{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "305I1C6KOo0B",
    "outputId": "7a46626f-eb36-4fba-f060-7985fb916ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "!pip install bitsandbytes transformers datasets peft faiss-cpu accelerate sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "539010fcfff94377b1fe3e0221c0df83",
      "048b0eb996ca43dfaa9fc496ccb3d58a",
      "232e13f925ec43b28926ce2be4771d87",
      "8aac1699515a40629f51d5f2e069576a",
      "cfefb124531d4a8abf6a6692ecdea0f3",
      "1b3994fed2244189bb07555794633012",
      "b2b6f57a27234b019a0e0938dc0b1016",
      "b03f7abe49184d85bb0ea3701ba23334",
      "a633da1ef3414d89b42bb2ce9e309f5a",
      "9e1317fb818246f49a974069506da610",
      "7999a22f6174459cbb8509a902e6a09d",
      "924e6cf1ac104938ba261c2a09b8f1b4",
      "01c7e7cb80f544c6860b37d805d793bc",
      "2e9aba755e064d598b624bb99f8589ff",
      "94c90c14a5af466a856e6ff0cb74c6d7",
      "d85efa012a594930bde24887399d77ac",
      "82c4b9ec3fc24a6dafdfcf56214d80f2",
      "00e515a4a4ea4e4baaad87af3483cbf0",
      "c0bb7a68e0734e609b69361cb0384306",
      "f08efea7c51b4f92b73e0e7006171dcd",
      "7dbfc7ff72e547d98454877305bd389a",
      "27cf4a934bc94197b380839cc457837f",
      "fa6ad10578f2469cb6c7d51b2efbbc45",
      "f1696e0046034df49713d17441da292b",
      "b969ff9c17c7404fba9631d7ba527ced",
      "d421ee3c11d44a5e8ecd000360b5bbed",
      "bdf08bd5f7f74b98834f4dcfa9875504",
      "2edbf563ab4840148bc1baba4ee36f91",
      "1f210116019949e48f77c4806e2d7f9d",
      "4d2bca781d254406a4dbe2a24352ca2e",
      "ce39ce6acec248a5bd2875e8546546c8",
      "6c9af648e6ef425c91d875bbda331d87",
      "2a8a01c5620d41698d71eab8088ee3d0",
      "2a8b5e120e9244be9e70f5898ff03a20",
      "bf3b525566274dd992fac769bce1aab4",
      "3635bfdcb8ab4abbbd112a1516cab8b6",
      "ee2a656dfeb3490fa23561332fd5f31f",
      "76a0edb85e6f43b899e5a55a0ec0502c",
      "c366ff289b8b4abca081cba71c5a3b21",
      "8346b6bf3f5f49758837f31539607b0f",
      "e35284f1660049f38529641b0226c8e7",
      "eed9c81cc3a34e2b81005e182559d381",
      "f645607d30404cc18714fed69bc69eec",
      "f264b9480a74495e8f8c7440d0740b97"
     ]
    },
    "id": "rusxLtzIOtsB",
    "outputId": "63fa39ff-1488-46a3-ff65-ad9d12f46323"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539010fcfff94377b1fe3e0221c0df83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924e6cf1ac104938ba261c2a09b8f1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6ad10578f2469cb6c7d51b2efbbc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8b5e120e9244be9e70f5898ff03a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset examples: 478\n",
      "Tokenizing samples...\n",
      "TOKEN LENGTH ANALYSIS\n",
      "Sample count: 100\n",
      "Min tokens: 50\n",
      "Median tokens: 56\n",
      "Max tokens: 64\n",
      "98th percentile: 62\n",
      "Rounded optimal seq length: 64\n",
      "\n",
      " Recommended max_seq_length = 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "\n",
    "DATASET_PATH = \"/content/dataset.json\"   # modify if needed\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"         # <-- changed to Qwen 2.5 1.5B tokenizer\n",
    "SAMPLE_SIZE = 100                        # how many examples to sample\n",
    "PERCENTILE = 98                          # target percentile\n",
    "ROUND_MULTIPLE = 64                      # round up to nearest 64\n",
    " \n",
    "# LOAD TOKENIZER\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# LOAD DATASET\n",
    "\n",
    "with open(DATASET_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded dataset examples: {len(data)}\")\n",
    "\n",
    "\n",
    "# SAMPLE SUBSET\n",
    "\n",
    "sampled = random.sample(data, min(SAMPLE_SIZE, len(data)))\n",
    "\n",
    "\n",
    "# BUILD TRAINING PROMPTS & MEASURE TOKEN LENGTHS\n",
    "\n",
    "def build_prompt(query, retrieved_chunks):\n",
    "    # retrieved_chunks = list of strings (we simulate 3 empty ones for now)\n",
    "    c1, c2, c3 = retrieved_chunks\n",
    "\n",
    "    return f\"\"\"\n",
    "[QUERY]\n",
    "{query}\n",
    "\n",
    "[RETRIEVED_CONTEXT]\n",
    "1. {c1}\n",
    "2. {c2}\n",
    "3. {c3}\n",
    "\n",
    "[INSTRUCTION]\n",
    "Generate a clinical report. Do NOT include disclaimers.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "lengths = []\n",
    "\n",
    "print(\"Tokenizing samples...\")\n",
    "for item in sampled:\n",
    "    q = item[\"query\"]\n",
    "\n",
    "    # For this stage we don't need real retrieval, so use blanks\n",
    "    prompt = build_prompt(q, [\"\", \"\", \"\"])\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    lengths.append(len(tokens))\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "\n",
    "# COMPUTE OPTIMAL MAX SEQ LEN\n",
    "\n",
    "raw_percentile = int(np.percentile(lengths, PERCENTILE))\n",
    "\n",
    "# Round up to nearest multiple of 64\n",
    "def round_up(x, multiple):\n",
    "    return ((x + multiple - 1) // multiple) * multiple\n",
    "\n",
    "optimal_seq = round_up(raw_percentile, ROUND_MULTIPLE)\n",
    "\n",
    "\n",
    "# RESULTS\n",
    "\n",
    "\n",
    "print(\"TOKEN LENGTH ANALYSIS\")\n",
    "\n",
    "print(\"Sample count:\", len(sampled))\n",
    "print(\"Min tokens:\", lengths.min())\n",
    "print(\"Median tokens:\", int(np.median(lengths)))\n",
    "print(\"Max tokens:\", lengths.max())\n",
    "print(f\"{PERCENTILE}th percentile:\", raw_percentile)\n",
    "print(\"Rounded optimal seq length:\", optimal_seq)\n",
    "\n",
    "\n",
    "print(f\"\\n Recommended max_seq_length = {optimal_seq}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvoABMm9OwKg",
    "outputId": "68fb62eb-2725-4e4e-9872-fb279fef4fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'query': \"I'm John, 35. Diagnosed with strep throat. What antibiotic is standard?\", 'report': 'Penicillin or Amoxicillin are first-line treatments for strep throat.'}\n",
      "1 {'query': 'My name is Lisa, 28. I have bacterial vaginosis. What medication works?', 'report': 'Metronidazole oral tablets or vaginal gel are standard treatments for BV.'}\n",
      "2 {'query': \"I'm David, 70. Diagnosed with pneumonia. What antibiotics are used?\", 'report': 'Community-acquired pneumonia is typically treated with Azithromycin or Amoxicillin.'}\n",
      "3 {'query': 'My name is Sarah, 42. I have shingles. What medication helps?', 'report': 'Antiviral medications like Acyclovir or Valacyclovir are prescribed for shingles.'}\n",
      "4 {'query': \"I'm Mike, 58. Diagnosed with gout attack. What treats the pain?\", 'report': 'Colchicine or NSAIDs like Indomethacin are used for acute gout pain.'}\n",
      "5 {'query': 'My name is Anna, 31. I have urinary tract infection. What antibiotic?', 'report': 'Nitrofurantoin or Trimethoprim-sulfamethoxazole are common for uncomplicated UTIs.'}\n",
      "6 {'query': \"I'm Tom, 65. Diagnosed with COPD. What maintenance medication?\", 'report': 'Long-acting bronchodilators like Tiotropium or combination inhalers are standard.'}\n",
      "7 {'query': 'My name is Maria, 50. I have rheumatoid arthritis. What DMARD?', 'report': 'Methotrexate is typically the first-line DMARD for rheumatoid arthritis.'}\n",
      "8 {'query': \"I'm James, 45. Diagnosed with acid reflux. What PPI?\", 'report': 'Omeprazole or Pantoprazole are common proton pump inhibitors for GERD.'}\n",
      "9 {'query': 'My name is Emily, 33. I have migraine. What abortive medication?', 'report': 'Triptans like Sumatriptan are first-line for acute migraine attacks.'}\n",
      " Missing report count: 0\n",
      " Indices with missing report: []\n",
      "Original size: 478\n",
      "Cleaned size: 478\n",
      "Removed: 0\n",
      " dataset.json cleaned & overwritten successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dataset.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, d in enumerate(data[:10]):\n",
    "    print(i, d)\n",
    "\n",
    "\n",
    "bad = [i for i, d in enumerate(data) if \"report\" not in d]\n",
    "print(\" Missing report count:\", len(bad))\n",
    "print(\" Indices with missing report:\", bad[:50])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "path = \"/content/dataset.json\"   # <-- update if different\n",
    "\n",
    "# Load\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Original size:\", len(data))\n",
    "\n",
    "# Clean broken entries\n",
    "cleaned = [d for d in data if \"report\" in d and isinstance(d[\"report\"], str) and len(d[\"report\"].strip()) > 0]\n",
    "\n",
    "print(\"Cleaned size:\", len(cleaned))\n",
    "print(\"Removed:\", len(data) - len(cleaned))\n",
    "\n",
    "# Overwrite the file\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(cleaned, f, indent=2)\n",
    "\n",
    "print(\" dataset.json cleaned & overwritten successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594,
     "referenced_widgets": [
      "76a3a85de4ce48958176c0f52692a08b",
      "f00c1b1755074d0096ae255da0f645cd",
      "c21100d93cfa45d6a2cc76b22d0b1970",
      "607a27b86621444da751fdc7b9cbda38",
      "c78f8de573054a148fef5dfb711192fa",
      "5a20db30277d483cba5b1e3eb171a172",
      "7cabc04f9c4046e587c77346a50049ac",
      "7c593afb9cc84045b22f1e921982aa59",
      "9e4a26f9a46c4cb7905c1380e222e72a",
      "3ef8513d376a4700a1788a44ade32d54",
      "73d889de72374122912b687a006afdf1",
      "316c812cd553498fb4cd70a13548fa69",
      "0d0529070278473ca57e52dcabcc3540",
      "12ac7de3ae2b43f19dbee85c3758ca45",
      "4cf236e297b042fb8a0a463fa48b5506",
      "2a5d01573bb042828d947e3d403f834c",
      "0d912eada7d14de286201447cb5d4812",
      "e65d97aed63f463f905042b0b35a57c8",
      "02d2a767894145e78d73fa71683c6603",
      "7c1184eb0d9548cbb221922e49a9bce8",
      "a5e237d5774249599b574ebb20393e7c",
      "0cf3bd14e4c644e3953abcac62093d26",
      "8e2a1f734bd84391832f2528826261b2",
      "2513dfd541c94ba99b4d229a763c18e3",
      "f5e76b6c63c94e20970b944157317022",
      "1c633b9f6a7b430a94ce6868bfcb4b69",
      "a3e6c99f23a045699cecd4a439fecd02",
      "acbad44d3ff048df86f112b62d06ab53",
      "681119a584cc48628f72ad195c098bb4",
      "2d7504153191410a8c4c5b505ea7b990",
      "c1709d74afc744d7a70bc506bb75fc07",
      "0d4dc4e8b9d6481ca53ca7b5bb03e7b5",
      "f351d9ef5aad4be583c2bfe0de886677",
      "3ded6784281344ce940e93e23a7920fb",
      "62e75b4d480e49a5b3872bf12a97d901",
      "1ba1f0828de1498aa908c515047ac88b",
      "48a6255fc6804e1f81d8903240663a43",
      "50431d6116a3431f987a63ac2dac01dc",
      "4fd768b486484bf5bd84d95c0299a9b2",
      "d823d5ad7ad346fca7192859a18538d0",
      "ca1285fba69b41dbbedc567e3726434f",
      "0a79358069034ba7a25a119f8f07328f",
      "d81f0dba4d114d12ad5776c8499015e4",
      "c6700b5a6a774545a8d5cbbcc5738cff",
      "914db80aacd44493af27b95c9918f769",
      "93a60e0e5a3f4a06806874aa727961f0",
      "8be3892963c6467cb8862aedbf01e0c6",
      "3143e9d390854bd1b80c3fd9cbf50ceb",
      "edcd504fcd4d4f5fb8cac07aa77cb85b",
      "b4ebc5c735034cbc81a8f77454c5300b",
      "394abe046b554bb893a28815d93ab00a",
      "e5f7149f1b6049c6be32062fd202b513",
      "08b269c4bb0040c0b7ebd62cf23593f3",
      "226fdd994cd540f4b02646b04cafd8cb",
      "2a988cce126a47e785595469de5cc0b3",
      "629223ebe2684616b10d69de2ab8a019",
      "536bcf1384404741b6a7ad55a05c0e7a",
      "649bb62550794a75b900500b7902e903",
      "cee71965e0d342ada1b8cbfa6679c139",
      "3e54522c43d04937b25604e7a02b0b91",
      "e8d292f2e2d84064aedd5c924969d6b2",
      "64a66fb8e8434577a253f076807032bf",
      "b57f7d1985ae4527b0014270c3e189a9",
      "0990ae1494a946bd852c1ec62ffe3f7b",
      "c8714713ac7c474681235dc5c3ca0992",
      "ff06516f7cc84dbb946832d3937a81dd",
      "e45f184c3bbb4019aad753bb49d3ff56",
      "6709528003da4f48b1dd83d2230432ab",
      "ae9fa30596514f6ab99f245f41ab56f7",
      "a49c459150ff451f889a77899baf51de",
      "bb170140bca744efa5f6a05ebc5cc309",
      "ca14b9ec4b9a44658e5978a09fc96b56",
      "21e011de1b664802b33764ab43339c8c",
      "c4004f51d5264a88ad7cc6e09b0dbc25",
      "690c9d81fda84d01bf32576366ba4fe2",
      "e6b1987b50cd4098ae29d45c7b0370d8",
      "ba1cc97c4791453ba52712b706c70f24",
      "138019db6ec6426eb541d1b69be16e67",
      "48e660e71d7446ecb7dc65b3dcd0a9f5",
      "e9704ab5cf3a42a9841c8a88c5f32d0e",
      "392bded59c514fa7bff657949bc7ba29",
      "0db187deb2174855bbd7ce596e8bc0da",
      "b470f07b7c0b4f5da0ddece3af8580aa",
      "4462f7845b56476b97e99ad3fa478732",
      "c02e83c2cd544706806a7e8336168452",
      "debbf3f239904e938a1d028a268956d2",
      "31745e86159049dbb707e631f63869ed",
      "28dac5af0e7a45e9a8b1220b97450e42",
      "ce9c587f9f2b4990ae77f7666092eab1",
      "c6e32c9d24a84270ac76fa0465f4002c",
      "28fbbbbdddc84835bb0762a929587d45",
      "3a4d92360a3c4361a0b56b2202813c8b",
      "f6af7929bb2a41dfa5902873a3b74261",
      "1678c9a4b80b4ff1ae934f582a0d47a4",
      "6696352df6524e5db90b4e559ca07584",
      "d015bf3a0136474da5fb47613b3a8ebf",
      "58af45b7dbb74df28f94d2f374e644b6",
      "659fe8489f524e61a9700810efbb5a5e",
      "0e78747ea00c4c7ab78dc13e8719dd48",
      "344d72c2ec7a4fc0adc5cc48322c30ce",
      "7d89701e1757445e954774f06e9b5c65",
      "fab8d052c3dd478e8dd972f4e5c3f017",
      "80df16329a744721892511a4209fdb5f",
      "243b50c5a3c748af802de413bbf74501",
      "9175a93975d14011a0eda51f61707970",
      "aadae6674dae4e6f8eece2f0c19fb16c",
      "23156b06c09f4238a86f7940076bda7b",
      "813ed63734dd47a5a30ec8426eb19984",
      "cdb8fcf413d24557ac1ea3764b172b71",
      "4003670d7c4c453087834835e2c307a6",
      "49eca76b52bc454f9cde1edc3701495e",
      "d35fa84125af40749116dccfb0c184db",
      "55586500e0eb4f12939eb42a4205d110",
      "2ed82f6e0c72493b965c5c8d21000e8f",
      "3f48375a64ff40808effe109f2437d0e",
      "bb3355f41dcc47259ea2eb39a69cb1da",
      "0a2c8edfd72f443aa7c6cb3fb775376b",
      "1c75ab0a052e4dd48cbcf1dead022587",
      "c97fe3a807114a8083c3cfa1beb29e8c",
      "c26f54692d3a44f69a2d05f5ed2742fd",
      "8ec0d6b0011a4a47a316c3b56e47ef10"
     ]
    },
    "id": "Pyy_T_FEQanS",
    "outputId": "30a7d492-5e9d-4514-b8c0-5a45c3f996a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset examples: 478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a3a85de4ce48958176c0f52692a08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316c812cd553498fb4cd70a13548fa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2a1f734bd84391832f2528826261b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded6784281344ce940e93e23a7920fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914db80aacd44493af27b95c9918f769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629223ebe2684616b10d69de2ab8a019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45f184c3bbb4019aad753bb49d3ff56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138019db6ec6426eb541d1b69be16e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9c587f9f2b4990ae77f7666092eab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344d72c2ec7a4fc0adc5cc48322c30ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eca76b52bc454f9cde1edc3701495e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 478 reports\n",
      "Tokenizing real training prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 80.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN LENGTH ANALYSIS (FULL RAG PROMPTS)\n",
      "Sample count: 100\n",
      "Min tokens: 111\n",
      "Median tokens: 130.0\n",
      "Max tokens: 148\n",
      "98th percentile: 148.0\n",
      "\n",
      "Recommended max_seq_length = 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " \n",
    "# 1) LOAD DATASET\n",
    " \n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Loaded dataset examples:\", len(data))\n",
    "\n",
    " \n",
    "# 2) BUILD FAISS INDEX FROM REPORTS ONLY\n",
    " \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "reports = [d[\"report\"] for d in data]\n",
    "report_embeddings = embedder.encode(reports, convert_to_numpy=True)\n",
    "\n",
    "dim = report_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(report_embeddings)\n",
    "index.add(report_embeddings)\n",
    "\n",
    "# map index \u2192 report text\n",
    "id2report = {i: reports[i] for i in range(len(reports))}\n",
    "\n",
    "print(\"FAISS index built with\", len(reports), \"reports\")\n",
    "\n",
    " \n",
    "# 3) RETRIEVAL FUNCTION (TOP-3)\n",
    " \n",
    "def retrieve_top3(query_text):\n",
    "    q_emb = embedder.encode([query_text], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    scores, hits = index.search(q_emb, 10)  # fetch top-10 to boost filtering\n",
    "    hits = hits[0].tolist()\n",
    "\n",
    "    # Simply pick top 3 reports for token estimation\n",
    "    retrieved = [id2report[i] for i in hits[:3]]\n",
    "    return retrieved\n",
    "\n",
    " \n",
    "# 4) BUILD FULL PROMPT (REAL TRAINING FORMAT)\n",
    " \n",
    "def build_prompt(query, chunks):\n",
    "    prompt = f\"\"\"\n",
    "[QUERY]\n",
    "{query}\n",
    "\n",
    "[RETRIEVED_CONTEXT]\n",
    "1. {chunks[0] if len(chunks)>0 else \"\"}\n",
    "2. {chunks[1] if len(chunks)>1 else \"\"}\n",
    "3. {chunks[2] if len(chunks)>2 else \"\"}\n",
    "\n",
    "[INSTRUCTION]\n",
    "You a medical assistant generate a clinical report. Do NOT include disclaimers.\n",
    "Given query and the relevant documents make a report using them.\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    " \n",
    "# 5) TOKENIZE WITH QWEN TOKENIZER\n",
    " \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "# Sample randomly to avoid tokenizing all 500\n",
    "sample_data = random.sample(data, min(100, len(data)))\n",
    "\n",
    "token_lengths = []\n",
    "\n",
    "print(\"Tokenizing real training prompts...\")\n",
    "\n",
    "for sample in tqdm(sample_data):\n",
    "    q = sample[\"query\"]\n",
    "    chunks = retrieve_top3(q)\n",
    "    full_prompt = build_prompt(q, chunks)\n",
    "\n",
    "    tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    token_lengths.append(tokens.input_ids.shape[1])\n",
    "\n",
    " \n",
    "# 6) ANALYZE LENGTHS\n",
    " \n",
    "import numpy as np\n",
    "\n",
    "arr = np.array(token_lengths)\n",
    "\n",
    "print(\"\\nTOKEN LENGTH ANALYSIS (FULL RAG PROMPTS)\")\n",
    "print(\"Sample count:\", len(arr))\n",
    "print(\"Min tokens:\", arr.min())\n",
    "print(\"Median tokens:\", np.median(arr))\n",
    "print(\"Max tokens:\", arr.max())\n",
    "print(\"98th percentile:\", np.percentile(arr, 98))\n",
    "\n",
    "recommended = int(np.percentile(arr, 98)) + 50  # safety pad\n",
    "print(\"\\nRecommended max_seq_length =\", recommended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c46d9ab9f6a64f02bf205c77a7dcc88d",
      "449544de5576470aa1eefa5d4e5d2c48",
      "ec7c22ef0bda45c2b730208500b49349",
      "c23162ca3cfa4b448c1bd865cf6c3cee",
      "01eedd3d2b37447a8528ee5a6e6d612e",
      "abca26989a6944d89af83c3bf6ac23ec",
      "815ce58c176a4c67a9ed3298a4168413",
      "8a0382364af7460ba5fcedb37e36adfa",
      "9e412603e68842a3b4faf942059d1fde",
      "f9b4edfeb970456c9fdc1bfd44851b19",
      "56c359d056e14a6fa95f51b976461e53",
      "1259df769fa84f58b21422a9cd3e37f8",
      "1a201a91ec1a4c879942a559ca2041ce",
      "1610a7d7f53b4e5bbecc76421ad632bb",
      "2f9fdadcaed245aaaf17ef6fa2d3ac5a",
      "366a9cb61f8d4129b9f4c3acd1cfb859",
      "db2c23d87a34403090017d4380a83fe0",
      "b3c16f09a8e4400a84f67d9b5ab4f55f",
      "4a6297df0db84644b478fc4d7574ae25",
      "6a1a3aa0374e4ba78609b9a12b403077",
      "14f14f15f2b246bd91a054e843c7b91c",
      "945fdcfa98384be18e22045f3f6c837b",
      "57f50c882cc241bb8b37cfa541bd4ef4",
      "7feb9d86090f4938b4f37b26653ea6aa",
      "810efa53e0ea4ea3ac5e65f1a55eaa5a",
      "a5b9d18339d4468a9bbf9b3a7aa1bb08",
      "6aac7ced4cec4317830b257fdb5405cd",
      "e983e978e2e14d26ae62e5895b539900",
      "b812821cb4ea4ac49d3f23e9958050ad",
      "b1db7d8f9e9d4863805e519dd5bf4db7",
      "2e0a0b30bee34d0895b20163573c8d51",
      "5b05a2cf798d4ead88914b594b5cec47",
      "fadcb804654641d18e6067cbc5e0f9ad",
      "b8ef96db58ea438199941631090acfa9",
      "0f3f88ba9b6d408c922b5b7f9a2a7dd5",
      "aca5e274bcbf46b6b348e864ce293c5f",
      "0dbf672235324c739433499498710a2f",
      "ba039551c42c4850ad7a9e79bb748b24",
      "cf8f3a884d274c70954247be138b8508",
      "1773f1ec9e6c483080697da5b7652a05",
      "6094d7549f68494d97de98c64977ad38",
      "25543eb3190f4dabbb8425bdee602ce0",
      "79fa0a40d6204a1eaa4f0f278c3b0890",
      "9fa160e0f32041c39a3fa5289b2dd11b",
      "2aa57270e6c04dc1a1227347d3180a06",
      "d9956b5444cf4f8386f2a49a2c4cdd78",
      "81787beca57d4a5782ba49d878239699",
      "25d8e5e3c6904cb0b2a1faa4f3beb379",
      "04d0fcafca524f94b403773d7c184b99",
      "ec2ce919f6664a919f15d062341e6eb0",
      "442e4e69fe894bc08c2a7a77ddf365fa",
      "ed57cdb02df943c3b2b10fef6972f3e0",
      "6c6c4a5c937c41f8aa8853b33d27151e",
      "63b9e9208ff0408dbac0351fff1cc61a",
      "3cebb7b6a812420c82512342971972a4",
      "210f7b922c7d4a71b9a5dbe47a2c127c",
      "c9b7e42e40754e29976a51cd121c9364",
      "4ec54977d60940d3994e77ee0a0ea385",
      "b2f001c05fda41c8818f173d50623650",
      "5de5de4e5a664da5a71afd9499668fde",
      "981d4b447ea3456096b61c4824debb56",
      "13e6def03aa841c18f4f74be91dead1b",
      "44ffc2b1a661404b8aef4a3f19f47249",
      "1ce0d975ecdd4800a0d062db96912b4d",
      "c55623937d68492988f8e191cbbe13ad",
      "48a1777f2cf64debb4ffbfa5b52e15ec",
      "7df704f811274da587bb34df62347652",
      "ee4c4d9e40d64e06b1afe3167f4916f2",
      "a44d3ce367b64cf7a93c8e40da126c37",
      "4cc66abaa10d4378a7f7c2928670145c",
      "0274fe135a184cebb46bd742e90e957a",
      "10210910539b441bbd5574c26b0f1c50",
      "2bfa810d6bf84cb187e999cbd0a31257",
      "de058858a5c1493382847b90149f251f",
      "156159906ac24b4e9de1abcac623e784",
      "e09e3439c5744100849fd2d7347c3dfd",
      "fce643934ba94519a9eb76242ac247b8",
      "37e61712800e48d2a8c6abbf8084ea45",
      "cbedc594a11c4ad991ad2cbabf511006",
      "c0752912b9f94d8b9434c608869fc3b2",
      "c32e9b9362ee4894948696db0de61e6d",
      "7a0672d9231a4227b2e8362efc80f420",
      "fbef284485b74a57ba5a807b6b154c91",
      "aa4bb6ad625c4351ab3f64003e428c59",
      "5e8377c137214d7e972bdb346b808a90",
      "78db93ceeb9042f9ad89247183af6bda",
      "50846886ff4c4bae9160ccf8e549b722",
      "8a0a4bd8d6234bd9baac9ff6ac19dbcb",
      "f290cb5afa7c47379c9074d683fd371b",
      "a56a00a2841f4e39be10fc4bf1928190",
      "e84170d72c7143c28967d1c65f80148a",
      "d4922036182d49c88c161ce1c697be80",
      "76916a069ab245e58bf7425cac75f2e3",
      "aa6ef265185b47a78f0d89f22c93d2cd",
      "aee6a2c3692048d6978c2033646aac13",
      "38679b68412a4409924c0f42d14aad53",
      "fe6e77f1564545e8bad763c2ae6d9372",
      "ddf8d8e934714b68906d815c91edfd03",
      "baed6b3c40e545be9f6b6e7245a54544",
      "30e7934483fd4d45913871ad23b2d810",
      "38dc21c4e97a4ae19db43e8112b14498",
      "b0c4c4921d5549fd910735a83688201e",
      "c3cb23dcec0c4e27a12f1cea578d9682",
      "d0417388f9754aadaff59fee3756e12e",
      "d55d40dc4d41490aaf0e9478be1bd1d6",
      "c5b5ecf006b7483595f0b371a0549a87",
      "a288444bbccb44f59adaf270fe69db11",
      "7d23dc1043104a4db6832a9a472af9ac",
      "b6e04201c34a40ec9a6b08fa8746f634",
      "df536d0bf8334b0f963294620f2b29c3",
      "eef69ba093bd43da83ddd7cc084999fb",
      "f1c3c2fec73f4732a25ad1e5795bcd11",
      "7778c0f04cd844a4a4b965ecf25631f5",
      "d3e474f0115d4958b3121ff72b685d10",
      "c0d1315b8b054c39a1f3b91248591f74",
      "148521abfd544f67aeed4ac591806ba3",
      "1b04a7758a784654a793dae68d2f4754",
      "c066c8d147e448df90bb3014b053ad35",
      "a592960754c54906bbbfb48618f268c1",
      "926e38fbc5ac421c92fecc283c08f863",
      "81f48f681e854e3aa51f6b3f37bd6d54",
      "1b2d45b0915b4726af2ad998083b291f",
      "27504d7836784cf8a9551d9c462961f0",
      "1571586210f443e9befb9423864301e4",
      "d67a60eef5db4ec8a10d5cc0cc93d04b",
      "3a59ed2a66c44e27a31fc470b54f24fb",
      "6aad8aeb857445c5a9e05f6a008e9e09",
      "90e83e35b11c4effab4e9bc1615efd19",
      "22030dae57c742608ce3d91ffb3fc933",
      "360cc14343e04b9aa31ece01d8a536ab",
      "05e6dfe7c85144eaa63295e62e4fca30",
      "5277dddba1dc4a48b3772d894effcf91",
      "7edbe49862b6472985de4139bb303fdd",
      "7af80878275d4ec9ae5c83dcf6cbe797",
      "b853befdb0b94fc29bf88dc1567200e4",
      "c00b3c3d411047b89b9642e7abdc8cb3",
      "90a60594e7094d47aff2112720532611",
      "9d9f4b9026814505903b13fe77a89d3b",
      "fe5f92f878704839bab8594f75788947",
      "d463e84e23d84de2bd5e8e895f995cf8",
      "5c0e9f4b9294499792da12260afde2a5",
      "472101ecebc94bee8baf1a4978fb3f52",
      "d8818b45b79a4be6b05bd4b32cf5fc36",
      "98f49e85f430425ab28d8ee05620c438",
      "7684ee154fc04019b42273e7e38b1ff4",
      "831c504bc2f14e6098ca04c7da64aa66",
      "152b36186f2a4cd9a6b03011f3f6b9c1",
      "be8074c85ffc49a6b196590a4e32ce75",
      "2bfd678042774ca48385fa58985c25aa",
      "bd0cdd2722c641998daa43e3090ceaf1",
      "c646e38d62a14ebda74d59828d73b0a1",
      "02becb1ab77c4850af87e4fd916cbff8",
      "0cbc586ef5274c1cbc331d009aeadb72",
      "c49cc6ae822b43e7952fdb56e9ff5d30",
      "cf9de1a5bbc24eeca729f9f36fda5a50",
      "046237ecaec642a4aec2c08218b292c9",
      "c41e79b806484b60b9c31cb599bdb950",
      "daeea28869874427ba348f8dc26da200",
      "1fafb7fa48b74e38b5d493d29b57b452",
      "61d9fdc0d541410da16273d6a0a4fdfb",
      "167fed1aa2e3453a88ce2323fc1b10fa",
      "1d8514895e6d482f8eedab2b7bd899aa",
      "76f423053e564e2cb73c2f87ff7f089c",
      "705a7f7fe146472db14a38998d238f4f",
      "19a6327582fc49d2a92ebd48b9e10266",
      "2bad7da575a141b5abc1dd1e76d0c0bc",
      "51f24f5c6a75421daaa8c945ba1ab4aa",
      "5c3bd25fb3334840bfb59445b951b782",
      "eee9fd33caaf42939ff5b601e7f778e5",
      "3cc9590555b84f059719c412c7bcea2d",
      "47d8c078c3e548f4b0ef814a47ccdbda",
      "ab98883433b642c0837c6fe4ebcc90a0",
      "43a2798acb08498483e4bf29e5380165",
      "91c143a0331547f58914a6a0a6b231f7",
      "0c158dbfa12e43d5b8687a62be2c2b05",
      "a8fe539128ca435d97a79671e087adc2",
      "b141093317be409598e984c4e489e19c",
      "41f6625a3261402cbcad7735eeeeaa7f",
      "a89ba0cf860b4a9e8178c35d1aaec0ea",
      "ad4afa308cfd4398849b614f9cdd8168",
      "aec112919323401fb155616fe88507a1",
      "56d3d6ca49674684a3250cc12ebf3816",
      "92b7297bf81149cbb690158895c39773",
      "b18f9157dedc4c268286a836841e74e7",
      "fe62e0f5bdd44932ac1a4a04c4839e6a",
      "06196eb515494df1a276c8caf34e2ce0",
      "7101c17389bf445494c1f7e08bbdfa22",
      "ffd8bc88fc4f47158e5aedbc1daad27c",
      "bfa85cf9c456420d8ec1dd31212e7e9c",
      "f617348a78e5424d932e1206854e0114",
      "38a2660f7b2e48ad8e214a9daca8d680",
      "eb991098cd8d4c848274ebdc7831f72a",
      "ee170195a3e941e0b6f1ec8fac92f3ba",
      "26a48b802ab349f7b7838acb829bd5e5",
      "88a2a80f0f7549ffbe823af1158d0e78",
      "7fef8f1125cd4cda96a8b8a5e564942c",
      "63866620df3a46cabc2ef8265cfa00af",
      "39edb8fd43aa4a9c8e9cea00e5f30a75",
      "ca6dcc6f77a84445aa93189f4cfc24b3",
      "b63aeaa9b5ad4fc3afaa713892987785",
      "50641e17319442af80f9ad07b35d5b9c",
      "bea9634a03e94c7e87328a1e8a8fddfd",
      "22e2f2a8409547d89d514ee04ec293d1",
      "b02a529e5bb743eb9f54e49cb835459d",
      "b6c2fb66655b459f9c67d78a760c298e",
      "1e791998662049f9873d3c70450b77b0",
      "6d6398d1a30f4cb1b5eded128baa4ee5",
      "31c9a0f66410420f89f8a8d6a7875568",
      "c50308fa2d3e4b6f9199a0bd99f68d32",
      "274ddf8aff7e4164bdbb15c4b60df6ff",
      "deba7d9f0630427690651f2da377c02d",
      "93a92e0da4c34599a97b70b3bf0569ab",
      "adf846238efc4bc8828c78cf0b3eb6d3",
      "6b59fbbf573e43e884df3c3360727fd7",
      "66e45d62092d4741a21842ac662ff291",
      "7e3fa15aae3e4bf0baa5d3230612f9a9",
      "0e53bba387154f0fb9c139f45bc93b40",
      "f37156b8d88849ca90d97b06e0a6f78b",
      "3164d640dbd84631af636b1cb57cdcd0",
      "b5faba98aa704d098d104ca1175c04be",
      "08a155f409c248818fbdbbb338380773",
      "b87023413a104807ab98ab9c2dcec8eb",
      "fe793dcb083845929b464a5a28247d6e",
      "3d0fae03d9574de4ade3a366df953266",
      "bf77af81524c4ddda2b0b7fcc7f7bc9d",
      "eaf0ef1eb5ac4753b5b60e56a49c69f2",
      "c1889137513f4669a0bce826df50421d",
      "735aaeaa1343435ea82eac6bbdf452ad",
      "30d57d4c7c4a4205ad27792a4feadfec",
      "883333eb85904411b6c47499f3d6aa79",
      "e7dfed6423344684bdfbebda634a8547",
      "b6872d08fa764296a819452ce28b0599",
      "9b0d9ad9136c47cca489b3e5d981ea3a",
      "a7d321532fbf4180993eb57a81e73513",
      "60db839ed55c43ddb5aaec8b56cf3929",
      "f59bb6168d1b429b81ccb0552e98246d",
      "cc99df88f38045d6a9b703841bb0e9ff",
      "acbf9ca2acb6408c866546f578403714",
      "8441b72f71c347388f23328991126aa6",
      "5d78dcab2c134910aed935f7c41982c8",
      "86d11a1d809c4b22aa5ffb9c65caccc4",
      "6bdd6769088744b680382fcf2b9ca2de",
      "6480600aab084dde9ab79837b9e70330",
      "08d2ef1b62444539aa5610d50b6c966d",
      "7e864e1f521e4780977a3def9f644909",
      "879dfbe5ee86473ca22988ed33458319",
      "6f7389b12d5247329d2bb2325b578760",
      "8f160d436c35438bbe80c7951c4e60f4",
      "dda4aa72fa7d41c68d0530ed40603761",
      "1e606385778343098cab4f7dc597db9a",
      "6f2dbb4c2db247119fa0f43d616e0945",
      "db9a396cf93e40378b303a12230046d3",
      "04872b7ac93a4d0fa30efc9cbfab33c4",
      "cddfd6f7de5d4b76a6edd425115b48a9",
      "3070e36551554c87af81949299d4a2aa",
      "66cb4346793b4336979d78ee4996653e",
      "ced0f8d8ae784d398b14552c284b15eb",
      "692e67fe8c6c4573a228e67b1e66ff60",
      "9aed786f79bb4bf8aca442af26d6b7fe",
      "5670ea34d6c04caa8e8eaad479badae5",
      "49ccb9ab7f1a45c094ae5e79078136bb",
      "0f84efec07d141c0a3e1b22b863d1b31",
      "8274e46775344ffea43b586c37c035f2",
      "88c05964c96a4e63b722506121254a78",
      "7761accb90a94f54aeee090049fb8460",
      "3f3d8432965643fcb6f447fb24250ac0",
      "e78ad950637e4489b5c199bf460c8c96",
      "ea9741c20fd447ae975f941e6e7cbc77",
      "ee380ad7e4374ffda4769b8b29d697f1",
      "8257a564ee6c4b66b3faa08926ab88df",
      "47db8422115c41aba900704dc74ff288",
      "740d33749d4f42b5a822459508c18784",
      "f2ac978604234031954e74e8ed50b1de",
      "532f1e01830a4386aa49d198e362016a",
      "9632ff97cf1648acae180bfad7403bb3",
      "db97111ad4a9407690eb7e1456df0ff8",
      "8f714c15c8f048cab09572d9e7643b13",
      "b4160bccdd32485d9a967f2982f0c28f",
      "46dc59e0552a45eda48aad3c05acb3bb",
      "2bbb4ba7ddfe4d1ca911a3c4aee20399",
      "fbffcf6fc73b4a35a60d3b329fd85736",
      "1d1c5493b6b748d4b3df562115dc9ff3",
      "d311442119cd4fe68fa89a1211279cc8",
      "019b9af2d7774ec299dea5a64905f22b",
      "cb6df7529fec490cb14592cba749ab91",
      "5e8a31301b2548a7a2fb7c3f4a3ee3e5"
     ]
    },
    "id": "f5Hy-5V3Jgrv",
    "outputId": "1bc82cd5-85db-4774-e425-2edd60817d2d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os, re, json, time, random, math, warnings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from cryptography.fernet import Fernet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Paths & basic config\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/qwen_finetune\"\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "\n",
    "DATASET_JSON = os.path.join(DRIVE_BASE, \"dataset.json\")  # put your dataset.json here\n",
    "ARTIFACTS_DIR = os.path.join(DRIVE_BASE, \"artifacts\")\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "FAISS_PATH = os.path.join(ARTIFACTS_DIR, \"reports_index.faiss\")\n",
    "META_PATH = os.path.join(ARTIFACTS_DIR, \"reports_metadata.json\")\n",
    "\n",
    "TRAIN_JSONL = os.path.join(ARTIFACTS_DIR, \"train_final.jsonl\")\n",
    "VAL_JSONL = os.path.join(ARTIFACTS_DIR, \"val_final.jsonl\")\n",
    "\n",
    "FINETUNED_DIR = os.path.join(DRIVE_BASE, \"qwen2p5_qlora_ft_merged\")\n",
    "os.makedirs(FINETUNED_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# User-tunable settings\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "BIOMED_NER = \"d4data/biomedical-ner-all\"\n",
    "DEFAULT_BASE_MODEL = \"Qwen/Qwen2.5-1.5B\"   \n",
    "RANDOM_SEED = 42\n",
    "SIM_THRESHOLD = 0.30\n",
    "TOPK = 3\n",
    "MASK_PROB = 0.60\n",
    "TEST_SIZE = 0.30\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_LENGTH = 200\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# Utility functions (masking / NER / simple PHI masking)\n",
    "\n",
    "PII_LABELS = {\"PERSON\",\"PATIENT\",\"LOCATION\",\"ADDRESS\",\"EMAIL\",\"PHONE\",\"ID\",\"DATE\"}\n",
    "\n",
    "def simple_regex_mask(text: str) -> str:\n",
    "    out = text\n",
    "    out = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", \"[NAME]\", out)\n",
    "    out = re.sub(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", \"[EMAIL]\", out)\n",
    "    out = re.sub(r\"\\b(\\+?\\d{1,3}[-.\\s]?)?(\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4})\\b\", \"[PHONE]\", out)\n",
    "    out = re.sub(r\"\\b\\d{1,2}[\\/\\-\\.\\s]\\d{1,2}[\\/\\-\\.\\s]\\d{2,4}\\b\", \"[DATE]\", out)\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline as hf_pipeline\n",
    "    ner_pipe = hf_pipeline(\"ner\", model=BIOMED_NER, tokenizer=BIOMED_NER, aggregation_strategy=\"simple\", device=0 if device==\"cuda\" else -1)\n",
    "    print(\"Loaded biomedical NER pipeline.\")\n",
    "except Exception as e:\n",
    "    ner_pipe = None\n",
    "    print(\"No biomedical NER available:\", e)\n",
    "\n",
    "def mask_with_ner(text: str) -> Tuple[str, Dict[str,str]]:\n",
    "    if ner_pipe is None:\n",
    "        return simple_regex_mask(text), {}\n",
    "    try:\n",
    "        res = ner_pipe(text)\n",
    "    except Exception:\n",
    "        return simple_regex_mask(text), {}\n",
    "    mask_map = {}\n",
    "    masked = text\n",
    "    spans = []\n",
    "    for ent in res:\n",
    "        label = (ent.get(\"entity_group\") or ent.get(\"entity\") or \"\").upper()\n",
    "        token = ent.get(\"word\") or ent.get(\"entity\") or ent.get(\"word\")\n",
    "        s = ent.get(\"start\", None); e = ent.get(\"end\", None)\n",
    "        if s is None or e is None:\n",
    "            continue\n",
    "        if label in PII_LABELS:\n",
    "            spans.append((s,e,label,token))\n",
    "    spans_sorted = sorted(spans, key=lambda x:x[0], reverse=True)\n",
    "    counters={}\n",
    "    for s,e,label,orig in spans_sorted:\n",
    "        counters[label] = counters.get(label,0)+1\n",
    "        token = f\"<PHI_{label}_{counters[label]}>\"\n",
    "        masked = masked[:s] + token + masked[e:]\n",
    "        mask_map[token] = orig\n",
    "    masked = simple_regex_mask(masked)\n",
    "    return masked, mask_map\n",
    "\n",
    "\n",
    "# Load dataset.json from Drive\n",
    "\n",
    "if not os.path.exists(DATASET_JSON):\n",
    "    raise FileNotFoundError(f\"Put your dataset.json at: {DATASET_JSON}\")\n",
    "\n",
    "with open(DATASET_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "    raw_data = json.load(fh)\n",
    "\n",
    "assert isinstance(raw_data, list), \"dataset.json must be a list of {query, report} pairs.\"\n",
    "print(f\"Loaded {len(raw_data)} examples\")\n",
    "\n",
    "\n",
    "# Build embedding model & FAISS on train reports\n",
    "\n",
    "print(\"Loading sentence-transformers embedder...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device=device)\n",
    "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dim:\", EMB_DIM)\n",
    "\n",
    "train_list, val_list = train_test_split(raw_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "print(f\"Train size: {len(train_list)}, Val size: {len(val_list)}\")\n",
    "\n",
    "print(\"Building FAISS index over train reports...\")\n",
    "reports = [rec[\"report\"] for rec in train_list]\n",
    "if len(reports) == 0:\n",
    "    raise RuntimeError(\"No reports found in train split.\")\n",
    "report_embs = embedder.encode(reports, convert_to_numpy=True).astype(\"float32\")\n",
    "faiss.normalize_L2(report_embs)\n",
    "index = faiss.IndexFlatIP(EMB_DIM)\n",
    "index.add(report_embs)\n",
    "faiss.write_index(index, FAISS_PATH)\n",
    "meta = [{\"id\": i, \"report\": reports[i], \"query\": train_list[i].get(\"query\",\"\")} for i in range(len(reports))]\n",
    "with open(META_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(meta, fh, indent=2)\n",
    "print(f\"FAISS index saved to {FAISS_PATH}. Metadata saved to {META_PATH}\")\n",
    "\n",
    "\n",
    "# Retrieval helper (entity/med boosting)\n",
    "\n",
    "def extract_entities_simple(text: str) -> Dict[str,List[str]]:\n",
    "    out = {}\n",
    "    if ner_pipe is not None:\n",
    "        try:\n",
    "            res = ner_pipe(text)\n",
    "            for ent in res:\n",
    "                label = (ent.get(\"entity_group\") or ent.get(\"entity\") or \"\").upper()\n",
    "                token = ent.get(\"word\") or ent.get(\"entity\") or ent.get(\"word\")\n",
    "                out.setdefault(label, []).append(token)\n",
    "            return out\n",
    "        except Exception:\n",
    "            pass\n",
    "    tokens = re.findall(r\"\\b[A-Za-z0-9\\-]{3,}\\b\", text)\n",
    "    words = [t.lower() for t in tokens]\n",
    "    disease_candidates = [w for w in words if re.search(r\"(itis|osis|emia|oma|drome|disease|infection|pneumonia|shingles|strep|gout|urinary|pneumonia)\", w)]\n",
    "    med_candidates = [w for w in words if re.search(r\"(cin|cillin|azole|cycline|mycin|vir|azole|statin|profen|ibuprofen|aspirin|colchicine|metronidazole|azithromycin|amoxicillin)\", w)]\n",
    "    if disease_candidates: out[\"DISEASE\"] = disease_candidates\n",
    "    if med_candidates: out[\"MEDICATION\"] = med_candidates\n",
    "    return out\n",
    "\n",
    "def retrieve_topk_reports(query: str, topk: int = TOPK, sim_threshold: float = SIM_THRESHOLD, fetch_k:int=None) -> List[Dict[str,Any]]:\n",
    "    if fetch_k is None:\n",
    "        fetch_k = max(10, topk * 4)\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D,I = index.search(q_emb, fetch_k)\n",
    "    hits = []\n",
    "    meta_local = meta\n",
    "    query_ents = extract_entities_simple(query)\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(meta_local):\n",
    "            continue\n",
    "        if float(score) < sim_threshold:\n",
    "            continue\n",
    "        rec = meta_local[int(idx)]\n",
    "        rec_ents = extract_entities_simple(rec.get(\"report\",\"\") + \" \" + (rec.get(\"query\",\"\") or \"\"))\n",
    "        overlap = 0\n",
    "        for ent_type in [\"MEDICATION\",\"DISEASE\",\"DISEASES\",\"DISEASE\"]:\n",
    "            if ent_type in query_ents and ent_type in rec_ents:\n",
    "                overlap += len(query_ents[ent_type]) * len(rec_ents[ent_type])\n",
    "        boosted_score = float(score) * (1.0 + overlap/10.0) if overlap > 0 else float(score)\n",
    "        hits.append({\"idx\": rec[\"id\"], \"report\": rec[\"report\"], \"score\": float(score), \"boosted_score\": boosted_score, \"orig_idx\": int(idx)})\n",
    "    hits = sorted(hits, key=lambda x: x[\"boosted_score\"], reverse=True)\n",
    "    return hits[:topk]\n",
    "\n",
    "\n",
    "# Build train/val jsonl files for finetuning\n",
    "\n",
    "def build_prompt(query: str, chunks: List[str]) -> str:\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(\"[QUERY]\")\n",
    "    prompt_lines.append(query.strip())\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"[RETRIEVED_CONTEXT]\")\n",
    "    for i, c in enumerate(chunks, start=1):\n",
    "        prompt_lines.append(f\"{i}. {c.strip()}\")\n",
    "    for i in range(len(chunks)+1, TOPK+1):\n",
    "        prompt_lines.append(f\"{i}. \")\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"[INSTRUCTION]\")\n",
    "    prompt_lines.append(\"Generate a clinical report. Do NOT include disclaimers.\")\n",
    "    prompt = \"\\n\".join(prompt_lines)\n",
    "    return prompt\n",
    "\n",
    "def create_dataset_jsonl(records: List[Dict[str,str]], out_path: str, mask_prob: float = MASK_PROB, sim_threshold:float = SIM_THRESHOLD):\n",
    "    lines = []\n",
    "    skipped = 0\n",
    "    for rec in records:\n",
    "        query = rec.get(\"query\",\"\").strip()\n",
    "        report = rec.get(\"report\",\"\").strip()\n",
    "        if not query or not report:\n",
    "            continue\n",
    "        hits = retrieve_topk_reports(query, topk=TOPK, sim_threshold=sim_threshold)\n",
    "        if len(hits) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        chunks = [h[\"report\"] for h in hits]\n",
    "        do_mask = (random.random() < mask_prob)\n",
    "        if do_mask:\n",
    "            masked_query, _ = mask_with_ner(query)\n",
    "            input_query = masked_query\n",
    "        else:\n",
    "            input_query = query\n",
    "        prompt = build_prompt(input_query, chunks)\n",
    "        obj = {\"input\": prompt, \"output\": report}\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\\n\".join(lines))\n",
    "    print(f\"Wrote {len(lines)} examples to {out_path} (skipped {skipped} examples due to no retrieval hits).\")\n",
    "    return len(lines)\n",
    "\n",
    "print(\"Creating train_final.jsonl and val_final.jsonl ...\")\n",
    "n_train = create_dataset_jsonl(train_list, TRAIN_JSONL, mask_prob=MASK_PROB, sim_threshold=SIM_THRESHOLD)\n",
    "n_val = create_dataset_jsonl(val_list, VAL_JSONL, mask_prob=0.0, sim_threshold=SIM_THRESHOLD)\n",
    "\n",
    "print(\"Sample training example:\")\n",
    "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        if i>0: break\n",
    "        print(json.dumps(json.loads(line), indent=2))\n",
    "\n",
    "\n",
    "# QLoRA finetuning using peft + bitsandbytes (8-bit quant)\n",
    "\n",
    "print(\"Preparing for QLoRA fine-tuning (8-bit quantization)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n",
    "\n",
    "def load_jsonl_to_dataset(jsonl_path: str):\n",
    "    records = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            ln = ln.strip()\n",
    "            if not ln: continue\n",
    "            obj = json.loads(ln)\n",
    "            records.append(obj)\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "train_ds = load_jsonl_to_dataset(TRAIN_JSONL)\n",
    "val_ds = load_jsonl_to_dataset(VAL_JSONL)\n",
    "print(\"Train/examples:\", len(train_ds), \"Val/examples:\", len(val_ds))\n",
    "\n",
    "def tokenize_and_mask(example):\n",
    "    inp = example[\"input\"]\n",
    "    tgt = example[\"output\"]\n",
    "    full_text = inp + \"\\n\\n\" + tgt + tokenizer.eos_token\n",
    "    tokenized_full = tokenizer(full_text, truncation=True, max_length=MAX_LENGTH)\n",
    "    input_tokenized = tokenizer(inp, truncation=True, max_length=MAX_LENGTH)\n",
    "    input_len = len(input_tokenized[\"input_ids\"])\n",
    "    labels = tokenized_full[\"input_ids\"].copy()\n",
    "    for i in range(input_len):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "    tokenized_full[\"labels\"] = labels\n",
    "    return tokenized_full\n",
    "\n",
    "train_ds_tokenized = train_ds.map(tokenize_and_mask, remove_columns=train_ds.column_names, num_proc=1)\n",
    "val_ds_tokenized = val_ds.map(tokenize_and_mask, remove_columns=val_ds.column_names, num_proc=1)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "\n",
    "print(\"Loading base model in 8-bit (this may take a while)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "target_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"]\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA adapters added. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = os.path.join(ARTIFACTS_DIR, f\"qlora_runs_{timestamp}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200 if len(train_ds_tokenized)>200 else max(1, len(train_ds_tokenized)//5),\n",
    "    save_steps=200 if len(train_ds_tokenized)>200 else max(1, len(train_ds_tokenized)//5),\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_tokenized,\n",
    "    eval_dataset=val_ds_tokenized if len(val_ds_tokenized)>0 else None,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "lora_weights_dir = os.path.join(ARTIFACTS_DIR, \"lora_weights\")\n",
    "os.makedirs(lora_weights_dir, exist_ok=True)\n",
    "model.save_pretrained(lora_weights_dir)\n",
    "print(\"Saved LoRA weights to:\", lora_weights_dir)\n",
    "\n",
    "print(\"Merging LoRA adapters into base model ...\")\n",
    "try:\n",
    "    model = model.merge_and_unload()\n",
    "except Exception as e:\n",
    "    print(\"merge_and_unload() not available or failed:\", e)\n",
    "    # proceed with saving current model (adapters present)\n",
    "\n",
    "print(\"Saving final model to:\", FINETUNED_DIR)\n",
    "model.save_pretrained(FINETUNED_DIR)\n",
    "tokenizer.save_pretrained(FINETUNED_DIR)\n",
    "print(\"Final fine-tuned model saved to Drive at:\", FINETUNED_DIR)\n",
    "\n",
    "artifacts_info = {\n",
    "    \"faiss_index\": FAISS_PATH,\n",
    "    \"meta\": META_PATH,\n",
    "    \"train_jsonl\": TRAIN_JSONL,\n",
    "    \"val_jsonl\": VAL_JSONL,\n",
    "    \"lora_weights\": lora_weights_dir,\n",
    "    \"merged_model\": FINETUNED_DIR,\n",
    "    \"notes\": {\n",
    "        \"sim_threshold\": SIM_THRESHOLD,\n",
    "        \"topk\": TOPK,\n",
    "        \"mask_prob\": MASK_PROB\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"artifacts_manifest.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(artifacts_info, fh, indent=2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}