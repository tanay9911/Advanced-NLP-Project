{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOvK8V0enFx1",
    "outputId": "f8b7e39e-ae8b-42ea-c685-1542312fa764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for negspacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers faiss-cpu ipywidgets cryptography sacremoses spacy negspacy bitsandbytes accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4a355d0ddbd1417dbe2e66e0a3a415a4",
      "384ee2933b684c4191e26afc8e3bd11a",
      "ba6f8ab93070474fa5a1ccecedfafcc9",
      "f95b8916c98d4bebb356c7fb6dea1b37",
      "467032d7ddf34f869302d791ceb1e49f",
      "2e1e2b0e9ff44e7996dffa768eb472f7",
      "0b7e45c4b3e14a3e9e35a5225fd3fe0a",
      "55ffda7737ae4a8aa2868ba708c01b63",
      "02eb97b483e64c85b5c87db749a99cd7",
      "18dc5f4df87347fd81076cf80d000fae",
      "6d9641f4e4824cd5816b9dcc073977c3",
      "9f51f70e557b4dd6b16c1449600a0034",
      "f3cf4feda1e54f2492a36320d395d6c2",
      "a8402f07be824de28a05c9bb51482c1c",
      "46ccb8a7dd5d43718731c07cc54841bb",
      "6159b76304e34445a0f5601130ec2cfb",
      "9c97498d585044dcb6942e7e42a001fa",
      "770b717d33434a21860ec0928d15e4d1",
      "c29dd518e5324bca9326a9a27d084811",
      "65a47172b9d64188851635c1918689ad",
      "8bb48781fb434fdf8fa0df290e692792",
      "263660e9e41f427392707cf507a81a69",
      "c94548f4730d4606a29f0f08b422c930",
      "a74ca6256fec47f38dbc035fe364e62a",
      "49dd073e64844587b1868079fb996964",
      "db6c2350d46644b6bceaa1f77659c765",
      "5d9bb80e0ed644ed8d751f8d59e0f0fb",
      "efee27a74a0149498e7e1ae02e40f7a6",
      "05bd1193bc7949d59a559d639866ee13",
      "2aa2cf56be424090a9737ae366a18d1b",
      "b82bc0b4da9b496196650b1ee11d7a5f",
      "0b44bd84758f4b118bcbf09d7ac89382",
      "aec9764d31ce4a67839c89ebb1bc9b0b",
      "2e5b945e6ca74bc2829ce9c5fc7f2a14",
      "c7b3f334b6844cce8094abc97af8a5e7",
      "385fee2a34244036a8ac0ed8a4267cbe",
      "c3c3e32030a944e4a829502005409c72",
      "ac03d684e20445bbb4377aa64104a061",
      "ff288233ac6d4ee9a05b008a7980d537",
      "99f3c7d66d18485a84b1940dad48f5ab",
      "24c52cb4f8da4277bc71e3888e7b05c4",
      "7416b1b1ec6b46acb866baa50966b8a2",
      "c5944d1630b241aca91979d850223f53",
      "81d0f5cc1a4e4ef3ac5f915eca9bb4fd",
      "64cbde20ca084c4980548aa777ad234a",
      "e04bbdcf00f34a439c0e3819ce925fb6",
      "7f9579b9edf346eebd5f1d37c2db2d11",
      "59129d0f399c403aae22d2ff2132215b",
      "2b0ed827cace41c6bd945d4b83849c09",
      "0fffffda888d41d2898d599aa85f449a",
      "a0cf4d9790db4e74befc88866f888591",
      "25dfd224dbfb443a8fdc8b1e2fbc6e16",
      "9c35f7b0363948b9bb2310db967bb521",
      "99e499f06be84f73a7a358999a0ecc86",
      "4d6c7320f08748b1aab7f8a24ff99b6e",
      "cde3a26c927740e5a441d2c6468d5608",
      "14263cdb898240c085e45c4de7cce13f",
      "ecaa92cc99ce47f6a6279e09e95ed68c",
      "5a50d1b7258f4e06b12659ada83fb269",
      "7edb8d1621604fb2923b91dfb6159bae",
      "e3c865cda02c49869ae26c6d902e4b81",
      "ca51310514f9466895bf37a62eb1a781",
      "256429a633c14e559582e9b1a02e049e",
      "feea5f34a9374c678610b976f99ef421",
      "f435f168e39d474a96fbc3403462e258",
      "9fac89d65b054080bae193e26f36ce5e",
      "0c954158ccf646438b1371a63a9d622c",
      "36a9103b333b4b34bf46108fb58c2a76",
      "2c17779d152b4ecaaca258b4bc696990",
      "1841998ab08340efb6d14da3141aea02",
      "e4a0a9fa2032475eb0983dd4c555c7db",
      "34f8cf206d8c43eebe196234d111963d",
      "5e53ff41131d49faaf722779b1ca3330",
      "99ad79306a66467abfb2e4536039bbf7",
      "5c1e612281fd4dd1857f7c7c827b441d",
      "5f8e9aededdd4d509e3b42064c7b0248",
      "e867925d583446fcafe03da485cf4833",
      "44819f51188c4f20802ea08bf9c4685d",
      "a4e21b84c19e45728c76bcd13f5a016f",
      "66fbe73b6efb4624928a265f2d0835fe",
      "dd82d5f23e034584aebee3fae539146b",
      "ebc637edf66f4aa1baf6ff70b4f0af83",
      "faa1acb2564d4853a83a69944c3dad27",
      "4313bda46aca4bd8b7bbe8c07d3ed31e",
      "436720c864164a0594b56fcfd0cf4c72",
      "0166f57d31104362947cd4b2657b3692",
      "4e2e09f62656424796515b241debcdd9",
      "0968672b94214859a0e3cc2165f13a6b",
      "501b91d3183e4aada74773de2bf674a8",
      "21863d086a8c400baefda9248f1186b9",
      "2b452649a0d14bddb53c7a140945c948",
      "f4912b5d6b644c6a8fad5915cb8194e4",
      "d4641d3bb2584eada67b3dd59e4f8659",
      "bfe2ef983ece48ddb490faeee03225ac",
      "e3f403b8b9ba473ebb8f01268f4601aa",
      "00ff778f7e38440cb8645e952b2b2301",
      "7e50fe339cc94f07baf12fef588cd2e6",
      "5b5509c8ba2544779af3ddfe5b689d43",
      "35b1e0277fa341b6bbf2d7eb2c17f686",
      "d29fffbb071b4a79acf222fc1857fb23",
      "88486510cb5841a19c4f9422b910d05e",
      "fd0cb2c0331845bfbf1e39c8a4c40f10",
      "aab8dba3480542b0888d4943bb5cfe27",
      "b4f21e7157e2470fbacf522e398979d9",
      "264bcfe4b6224cdcb285aed6b93f4cb8",
      "28e1030def1b4420bd507f060be552e5",
      "f0b0dd0010154b468b292f8520b5bf98",
      "cc8ca37dd6014d7fbe3c98fdbdc94dd9",
      "05de1219c31d4149b20a488d16a419ae",
      "c8c42c7c0e25401fbe10de9bf5e6b716",
      "15cc37576fef4482a2d82b857a729dbd",
      "8b206424cc1240fbac17bb9836b1a33b",
      "1e7aa7b266a04eb8b451556c082daf74",
      "a563d67aaa884693a9fef5084be89466",
      "411891b41de24eca8c7cac81c32405e6",
      "3a32778678c3435bb1204632f1c574dd",
      "95573d221b7a44898e21e2555370daf3",
      "d39366c0bb884a259467135e2c2b4a28",
      "8af32ed0e5ff4ace82078d279615ea89",
      "c4bcf296277b4c7c822c13d8c4bacea0",
      "853b0af1b8f14189b7e57b63a50e3d48"
     ]
    },
    "id": "71ZUilAsm_Ts",
    "outputId": "2bc9d275-3d7c-4c1f-fc85-5492cf222d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG data saved: 334 items -> /content/drive/MyDrive/qwen_finetune/rag-data.json\n",
      "User queries saved: 144 items -> /content/drive/MyDrive/qwen_finetune/user-query.json\n",
      "Device: cuda\n",
      "Encrypted RAG saved to /content/drive/MyDrive/User_data_rag/rag_data.json.enc\n",
      "Loaded 334 RAG documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a355d0ddbd1417dbe2e66e0a3a415a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f51f70e557b4dd6b16c1449600a0034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94548f4730d4606a29f0f08b422c930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5b945e6ca74bc2829ce9c5fc7f2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cbde20ca084c4980548aa777ad234a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde3a26c927740e5a441d2c6468d5608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c954158ccf646438b1371a63a9d622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44819f51188c4f20802ea08bf9c4685d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501b91d3183e4aada74773de2bf674a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29fffbb071b4a79acf222fc1857fb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cc37576fef4482a2d82b857a729dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 384\n",
      "FAISS index built and saved at /content/drive/MyDrive/User_data_rag/rag_faiss.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Qwen model loaded in 8-bit.\n",
      "[1/144] Query processed. Consent=True. Final answer length: 118 chars.\n",
      "[2/144] Query processed. Consent=False. Final answer length: 98 chars.\n",
      "[3/144] Query processed. Consent=True. Final answer length: 83 chars.\n",
      "[4/144] Query processed. Consent=False. Final answer length: 35 chars.\n",
      "[5/144] Query processed. Consent=True. Final answer length: 95 chars.\n",
      "[6/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
      "[7/144] Query processed. Consent=False. Final answer length: 84 chars.\n",
      "[8/144] Query processed. Consent=False. Final answer length: 74 chars.\n",
      "[9/144] Query processed. Consent=True. Final answer length: 70 chars.\n",
      "[10/144] Query processed. Consent=True. Final answer length: 60 chars.\n",
      "[11/144] Query processed. Consent=True. Final answer length: 101 chars.\n",
      "[12/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
      "[13/144] Query processed. Consent=True. Final answer length: 87 chars.\n",
      "[14/144] Query processed. Consent=False. Final answer length: 76 chars.\n",
      "[15/144] Query processed. Consent=False. Final answer length: 59 chars.\n",
      "[16/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[17/144] Query processed. Consent=True. Final answer length: 67 chars.\n",
      "[18/144] Query processed. Consent=True. Final answer length: 88 chars.\n",
      "[19/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
      "[20/144] Query processed. Consent=False. Final answer length: 96 chars.\n",
      "[21/144] Query processed. Consent=False. Final answer length: 82 chars.\n",
      "[22/144] Query processed. Consent=False. Final answer length: 72 chars.\n",
      "[23/144] Query processed. Consent=False. Final answer length: 93 chars.\n",
      "[24/144] Query processed. Consent=False. Final answer length: 98 chars.\n",
      "[25/144] Query processed. Consent=False. Final answer length: 52 chars.\n",
      "[26/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
      "[27/144] Query processed. Consent=False. Final answer length: 100 chars.\n",
      "[28/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
      "[29/144] Query processed. Consent=True. Final answer length: 124 chars.\n",
      "[30/144] Query processed. Consent=True. Final answer length: 106 chars.\n",
      "[31/144] Query processed. Consent=False. Final answer length: 94 chars.\n",
      "[32/144] Query processed. Consent=False. Final answer length: 78 chars.\n",
      "[33/144] Query processed. Consent=False. Final answer length: 92 chars.\n",
      "[34/144] Query processed. Consent=True. Final answer length: 79 chars.\n",
      "[35/144] Query processed. Consent=False. Final answer length: 114 chars.\n",
      "[36/144] Query processed. Consent=True. Final answer length: 103 chars.\n",
      "[37/144] Query processed. Consent=False. Final answer length: 34 chars.\n",
      "[38/144] Query processed. Consent=True. Final answer length: 102 chars.\n",
      "[39/144] Query processed. Consent=False. Final answer length: 93 chars.\n",
      "[40/144] Query processed. Consent=True. Final answer length: 73 chars.\n",
      "[41/144] Query processed. Consent=False. Final answer length: 100 chars.\n",
      "[42/144] Query processed. Consent=False. Final answer length: 88 chars.\n",
      "[43/144] Query processed. Consent=True. Final answer length: 58 chars.\n",
      "[44/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
      "[45/144] Query processed. Consent=False. Final answer length: 100 chars.\n",
      "[46/144] Query processed. Consent=False. Final answer length: 41 chars.\n",
      "[47/144] Query processed. Consent=True. Final answer length: 71 chars.\n",
      "[48/144] Query processed. Consent=False. Final answer length: 76 chars.\n",
      "[49/144] Query processed. Consent=True. Final answer length: 61 chars.\n",
      "[50/144] Query processed. Consent=True. Final answer length: 98 chars.\n",
      "[51/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
      "[52/144] Query processed. Consent=False. Final answer length: 95 chars.\n",
      "[53/144] Query processed. Consent=False. Final answer length: 93 chars.\n",
      "[54/144] Query processed. Consent=True. Final answer length: 76 chars.\n",
      "[55/144] Query processed. Consent=False. Final answer length: 85 chars.\n",
      "[56/144] Query processed. Consent=True. Final answer length: 96 chars.\n",
      "[57/144] Query processed. Consent=True. Final answer length: 81 chars.\n",
      "[58/144] Query processed. Consent=False. Final answer length: 107 chars.\n",
      "[59/144] Query processed. Consent=False. Final answer length: 101 chars.\n",
      "[60/144] Query processed. Consent=False. Final answer length: 92 chars.\n",
      "[61/144] Query processed. Consent=False. Final answer length: 78 chars.\n",
      "[62/144] Query processed. Consent=False. Final answer length: 73 chars.\n",
      "[63/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[64/144] Query processed. Consent=False. Final answer length: 74 chars.\n",
      "[65/144] Query processed. Consent=False. Final answer length: 82 chars.\n",
      "[66/144] Query processed. Consent=False. Final answer length: 92 chars.\n",
      "[67/144] Query processed. Consent=False. Final answer length: 99 chars.\n",
      "[68/144] Query processed. Consent=True. Final answer length: 113 chars.\n",
      "[69/144] Query processed. Consent=False. Final answer length: 106 chars.\n",
      "[70/144] Query processed. Consent=True. Final answer length: 66 chars.\n",
      "[71/144] Query processed. Consent=True. Final answer length: 92 chars.\n",
      "[72/144] Query processed. Consent=False. Final answer length: 70 chars.\n",
      "[73/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
      "[74/144] Query processed. Consent=True. Final answer length: 127 chars.\n",
      "[75/144] Query processed. Consent=True. Final answer length: 74 chars.\n",
      "[76/144] Query processed. Consent=True. Final answer length: 105 chars.\n",
      "[77/144] Query processed. Consent=True. Final answer length: 100 chars.\n",
      "[78/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
      "[79/144] Query processed. Consent=True. Final answer length: 105 chars.\n",
      "[80/144] Query processed. Consent=True. Final answer length: 68 chars.\n",
      "[81/144] Query processed. Consent=True. Final answer length: 95 chars.\n",
      "[82/144] Query processed. Consent=False. Final answer length: 110 chars.\n",
      "[83/144] Query processed. Consent=True. Final answer length: 113 chars.\n",
      "[84/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[85/144] Query processed. Consent=False. Final answer length: 40 chars.\n",
      "[86/144] Query processed. Consent=True. Final answer length: 66 chars.\n",
      "[87/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
      "[88/144] Query processed. Consent=False. Final answer length: 70 chars.\n",
      "[89/144] Query processed. Consent=True. Final answer length: 71 chars.\n",
      "[90/144] Query processed. Consent=True. Final answer length: 70 chars.\n",
      "[91/144] Query processed. Consent=True. Final answer length: 64 chars.\n",
      "[92/144] Query processed. Consent=False. Final answer length: 100 chars.\n",
      "[93/144] Query processed. Consent=True. Final answer length: 105 chars.\n",
      "[94/144] Query processed. Consent=False. Final answer length: 79 chars.\n",
      "[95/144] Query processed. Consent=False. Final answer length: 0 chars.\n",
      "[96/144] Query processed. Consent=False. Final answer length: 104 chars.\n",
      "[97/144] Query processed. Consent=False. Final answer length: 64 chars.\n",
      "[98/144] Query processed. Consent=True. Final answer length: 102 chars.\n",
      "[99/144] Query processed. Consent=False. Final answer length: 66 chars.\n",
      "[100/144] Query processed. Consent=False. Final answer length: 58 chars.\n",
      "[101/144] Query processed. Consent=False. Final answer length: 72 chars.\n",
      "[102/144] Query processed. Consent=True. Final answer length: 0 chars.\n",
      "[103/144] Query processed. Consent=False. Final answer length: 71 chars.\n",
      "[104/144] Query processed. Consent=False. Final answer length: 67 chars.\n",
      "[105/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[106/144] Query processed. Consent=False. Final answer length: 74 chars.\n",
      "[107/144] Query processed. Consent=True. Final answer length: 96 chars.\n",
      "[108/144] Query processed. Consent=False. Final answer length: 94 chars.\n",
      "[109/144] Query processed. Consent=True. Final answer length: 85 chars.\n",
      "[110/144] Query processed. Consent=True. Final answer length: 80 chars.\n",
      "[111/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
      "[112/144] Query processed. Consent=False. Final answer length: 125 chars.\n",
      "[113/144] Query processed. Consent=False. Final answer length: 84 chars.\n",
      "[114/144] Query processed. Consent=False. Final answer length: 80 chars.\n",
      "[115/144] Query processed. Consent=True. Final answer length: 125 chars.\n",
      "[116/144] Query processed. Consent=True. Final answer length: 101 chars.\n",
      "[117/144] Query processed. Consent=False. Final answer length: 73 chars.\n",
      "[118/144] Query processed. Consent=True. Final answer length: 86 chars.\n",
      "[119/144] Query processed. Consent=False. Final answer length: 67 chars.\n",
      "[120/144] Query processed. Consent=False. Final answer length: 84 chars.\n",
      "[121/144] Query processed. Consent=False. Final answer length: 31 chars.\n",
      "[122/144] Query processed. Consent=False. Final answer length: 102 chars.\n",
      "[123/144] Query processed. Consent=True. Final answer length: 116 chars.\n",
      "[124/144] Query processed. Consent=False. Final answer length: 107 chars.\n",
      "[125/144] Query processed. Consent=True. Final answer length: 98 chars.\n",
      "[126/144] Query processed. Consent=True. Final answer length: 133 chars.\n",
      "[127/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[128/144] Query processed. Consent=False. Final answer length: 96 chars.\n",
      "[129/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
      "[130/144] Query processed. Consent=False. Final answer length: 93 chars.\n",
      "[131/144] Query processed. Consent=True. Final answer length: 75 chars.\n",
      "[132/144] Query processed. Consent=False. Final answer length: 78 chars.\n",
      "[133/144] Query processed. Consent=True. Final answer length: 116 chars.\n",
      "[134/144] Query processed. Consent=True. Final answer length: 125 chars.\n",
      "[135/144] Query processed. Consent=True. Final answer length: 69 chars.\n",
      "[136/144] Query processed. Consent=False. Final answer length: 79 chars.\n",
      "[137/144] Query processed. Consent=True. Final answer length: 74 chars.\n",
      "[138/144] Query processed. Consent=False. Final answer length: 107 chars.\n",
      "[139/144] Query processed. Consent=False. Final answer length: 55 chars.\n",
      "[140/144] Query processed. Consent=False. Final answer length: 80 chars.\n",
      "[141/144] Query processed. Consent=False. Final answer length: 37 chars.\n",
      "[142/144] Query processed. Consent=True. Final answer length: 120 chars.\n",
      "[143/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
      "[144/144] Query processed. Consent=True. Final answer length: 80 chars.\n",
      "All results saved to /content/drive/MyDrive/User_data_rag/user_query_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DATASET_JSON = \"/content/drive/MyDrive/qwen_finetune/dataset.json\"\n",
    "RAG_JSON = \"/content/drive/MyDrive/qwen_finetune/rag-data.json\"\n",
    "USER_QUERY_JSON = \"/content/drive/MyDrive/qwen_finetune/user-query.json\"\n",
    "\n",
    "# Load original dataset\n",
    "with open(DATASET_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Shuffle for random split\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# 70% RAG, 30% user queries\n",
    "split_idx = int(len(dataset) * 0.7)\n",
    "rag_data = dataset[:split_idx]\n",
    "user_queries = dataset[split_idx:]\n",
    "\n",
    "# Save RAG data\n",
    "with open(RAG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rag_data, f, indent=2)\n",
    "print(f\"RAG data saved: {len(rag_data)} items -> {RAG_JSON}\")\n",
    "\n",
    "# Save user queries\n",
    "with open(USER_QUERY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_queries, f, indent=2)\n",
    "print(f\"User queries saved: {len(user_queries)} items -> {USER_QUERY_JSON}\")\n",
    "\n",
    "\n",
    " \n",
    "# RAG PIPELINE \n",
    " \n",
    "\n",
    "import os, json, re, time, random, warnings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "\n",
    "# Paths & configuration\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/User_data_rag\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "RAG_JSON = \"/content/drive/MyDrive/qwen_finetune/rag-data.json\"\n",
    "ENCRYPTED_RAG_PATH = os.path.join(BASE_DIR, \"rag_data.json.enc\")\n",
    "USER_QUERY_JSON = \"/content/drive/MyDrive/qwen_finetune/user-query.json\"\n",
    "\n",
    "# LOADING QWEN FINETUNED MODEL\n",
    "FINETUNED_MODEL_DIR = \"/content/drive/MyDrive/qwen_finetune/qwen2p5_qlora_ft_merged\"\n",
    "\n",
    "FAISS_INDEX_PATH = os.path.join(BASE_DIR, \"rag_faiss.index\")\n",
    "FERNET_KEY_PATH = os.path.join(BASE_DIR, \"fernet.key\")\n",
    "AUDIT_LOG_PATH = os.path.join(BASE_DIR, \"audit.log\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TOPK = 3\n",
    "SIM_THRESHOLD = 0.3\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "PII_LABELS = {\"PERSON\",\"PATIENT\",\"LOCATION\",\"ADDRESS\",\"EMAIL\",\"PHONE\",\"ID\",\"DATE\"}\n",
    "MEDICAL_ENTS = {\"MEDICATION\",\"DISEASE\",\"DISEASES\",\"SYMPTOM\",\"SYMPTOMS\",\"SIGN_SYMPTOM\",\"PROCEDURE\",\"LABS\",\"VITALS\",\"AGE\"}\n",
    "\n",
    "\n",
    "# Encryption helpers\n",
    "def get_or_create_fernet_key(path: str = FERNET_KEY_PATH) -> bytes:\n",
    "    if os.path.exists(path):\n",
    "        return open(path,\"rb\").read()\n",
    "    key = Fernet.generate_key()\n",
    "    open(path,\"wb\").write(key)\n",
    "    return key\n",
    "\n",
    "FERNET_KEY = get_or_create_fernet_key()\n",
    "FERNET = Fernet(FERNET_KEY)\n",
    "\n",
    "def encrypt_bytes(b: bytes) -> bytes:\n",
    "    return FERNET.encrypt(b)\n",
    "\n",
    "def decrypt_bytes(b: bytes) -> bytes:\n",
    "    return FERNET.decrypt(b)\n",
    "\n",
    "def write_encrypted_json(id_int: int, payload: Dict[str,Any]) -> str:\n",
    "    path = os.path.join(BASE_DIR, f\"{id_int}.json.enc\")\n",
    "    raw = json.dumps(payload, default=str).encode(\"utf-8\")\n",
    "    cipher = encrypt_bytes(raw)\n",
    "    with open(path,\"wb\") as fh:\n",
    "        fh.write(cipher)\n",
    "    return path\n",
    "\n",
    "def write_encrypted_rag(rag_data: List[Dict[str,Any]], path: str = ENCRYPTED_RAG_PATH):\n",
    "    raw = json.dumps(rag_data, default=str).encode(\"utf-8\")\n",
    "    with open(path,\"wb\") as fh:\n",
    "        fh.write(encrypt_bytes(raw))\n",
    "    print(f\"Encrypted RAG saved to {path}\")\n",
    "\n",
    "def load_decrypted_rag(path: str = ENCRYPTED_RAG_PATH) -> List[Dict[str,Any]]:\n",
    "    with open(path,\"rb\") as fh:\n",
    "        decrypted = decrypt_bytes(fh.read())\n",
    "    return json.loads(decrypted)\n",
    "\n",
    "\n",
    "# Encrypting RAG data \n",
    "if not os.path.exists(ENCRYPTED_RAG_PATH):\n",
    "    with open(RAG_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "        rag_data = json.load(fh)\n",
    "    write_encrypted_rag(rag_data)\n",
    "else:\n",
    "    print(\"Encrypted RAG already exists.\")\n",
    "\n",
    "# Load RAG and build FAISS\n",
    "rag_data = load_decrypted_rag()\n",
    "print(f\"Loaded {len(rag_data)} RAG documents.\")\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device=DEVICE)\n",
    "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dim:\", EMB_DIM)\n",
    "\n",
    "index = faiss.IndexFlatIP(EMB_DIM)\n",
    "texts = [rec[\"report\"] for rec in rag_data]\n",
    "embs = embedder.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
    "faiss.normalize_L2(embs)\n",
    "index.add(embs)\n",
    "faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "print(f\"FAISS index built and saved at {FAISS_INDEX_PATH}\")\n",
    "\n",
    "META_PATH = os.path.join(BASE_DIR, \"rag_metadata.json\")\n",
    "meta = [{\"id\": i, \"report\": rag_data[i][\"report\"], \"query\": rag_data[i].get(\"query\",\"\")} for i in range(len(rag_data))]\n",
    "with open(META_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(meta, fh, indent=2)\n",
    "\n",
    "\n",
    " \n",
    "# LOAD FINETUNED QWEN MODEL\n",
    " \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_DIR, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINETUNED_MODEL_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned Qwen model loaded in 8-bit.\")\n",
    "\n",
    "\n",
    "# NER + PII masking\n",
    "\n",
    "try:\n",
    "    ner_pipe = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    ner_pipe = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def simple_mask(text: str) -> Tuple[str, Dict[str,str]]:\n",
    "    mask_map = {}\n",
    "    out = text\n",
    "\n",
    "    names = re.findall(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", out)\n",
    "    for i, n in enumerate(names, 1):\n",
    "        token = f\"<PHI_PERSON_{i}>\"\n",
    "        mask_map[token] = n\n",
    "        out = out.replace(n, token)\n",
    "\n",
    "    emails = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", out)\n",
    "    for i, e in enumerate(emails, 1):\n",
    "        token = f\"<PHI_EMAIL_{i}>\"\n",
    "        mask_map[token] = e\n",
    "        out = out.replace(e, token)\n",
    "\n",
    "    phones = re.findall(r\"\\b(\\+?\\d[\\d\\-\\s]{7,}\\d)\\b\", out)\n",
    "    for i, p in enumerate(phones, 1):\n",
    "        token = f\"<PHI_PHONE_{i}>\"\n",
    "        mask_map[token] = p\n",
    "        out = out.replace(p, token)\n",
    "\n",
    "    dates = re.findall(r\"\\b\\d{1,2}[\\/\\-\\.\\s]\\d{1,2}[\\/\\-\\.\\s]\\d{2,4}\\b\", out)\n",
    "    for i, d in enumerate(dates, 1):\n",
    "        token = f\"<PHI_DATE_{i}>\"\n",
    "        mask_map[token] = d\n",
    "        out = out.replace(d, token)\n",
    "\n",
    "    return out, mask_map\n",
    "\n",
    "\n",
    "def extract_entities(text: str) -> Dict[str, Any]:\n",
    "    doc = ner_pipe(text)\n",
    "    ents = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_.upper()\n",
    "        ents.setdefault(label, []).append({\n",
    "            \"text\": ent.text,\n",
    "            \"start\": ent.start_char,\n",
    "            \"end\": ent.end_char\n",
    "        })\n",
    "    return ents\n",
    "\n",
    "\n",
    "NEG_WORDS = r\"\\b(no|not|denies?|without|absent|negative for)\\b\"\n",
    "def mark_negation(entities: Dict[str,Any], text: str, window_chars:int=40) -> Dict[str,Any]:\n",
    "    for label, recs in entities.items():\n",
    "        new_recs = []\n",
    "        for r in recs:\n",
    "            r[\"negated\"] = False\n",
    "            s = r.get(\"start\", 0)\n",
    "            context = text[max(0, s-window_chars):s].lower()\n",
    "            if re.search(NEG_WORDS, context):\n",
    "                r[\"negated\"] = True\n",
    "            new_recs.append(r)\n",
    "        entities[label] = new_recs\n",
    "    return entities\n",
    "\n",
    "\n",
    "# RAG retrieval\n",
    "\n",
    "def retrieve_topk(query_vec: np.ndarray, topk:int=TOPK, sim_threshold:float=SIM_THRESHOLD):\n",
    "    q = query_vec.reshape(1, -1).astype(\"float32\")\n",
    "    faiss.normalize_L2(q)\n",
    "    D, I = index.search(q, topk * 4)\n",
    "    retrieved = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(meta):\n",
    "            continue\n",
    "        if score < sim_threshold:\n",
    "            continue\n",
    "        retrieved.append(meta[int(idx)])\n",
    "        if len(retrieved) >= topk:\n",
    "            break\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "# Prompt builder \n",
    "\n",
    "def build_prompt(query: str, retrieved: List[Dict[str,Any]]) -> str:\n",
    "    lines = [\"[QUERY]\", query, \"\", \"[RETRIEVED_CONTEXT]\"]\n",
    "\n",
    "    for i, r in enumerate(retrieved, 1):\n",
    "        lines.append(f\"{i}. {r['report']}\")\n",
    "\n",
    "    for i in range(len(retrieved) + 1, TOPK + 1):\n",
    "        lines.append(f\"{i}. \")\n",
    "\n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"[INSTRUCTION]\",\n",
    "        \"Generate a clinical report. Do NOT include disclaimers.\"\n",
    "    ])\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Model generation \n",
    "\n",
    "def generate_from_model(prompt: str, max_tokens: int = 256) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0]\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out[prompt_len:]\n",
    "\n",
    "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Run pipeline\n",
    "\n",
    "with open(USER_QUERY_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "    user_queries = json.load(fh)\n",
    "\n",
    "results = []\n",
    "num_queries = len(user_queries)\n",
    "num_masked = int(num_queries * 0.6)\n",
    "\n",
    "shuffled_indices = list(range(num_queries))\n",
    "random.shuffle(shuffled_indices)\n",
    "masked_indices = set(shuffled_indices[:num_masked])\n",
    "\n",
    "for i, rec in enumerate(user_queries, 1):\n",
    "    query_text = rec.get(\"query\", \"\").strip()\n",
    "    consent_flag = False if (i - 1) in masked_indices else True\n",
    "\n",
    "    entities = extract_entities(query_text)\n",
    "    entities = mark_negation(entities, query_text)\n",
    "\n",
    "    if not consent_flag:\n",
    "        processed_query, mask_map = simple_mask(query_text)\n",
    "    else:\n",
    "        processed_query = query_text\n",
    "        mask_map = {}\n",
    "\n",
    "    q_vec = embedder.encode([processed_query], convert_to_numpy=True).astype(\"float32\")[0]\n",
    "\n",
    "    retrieved = retrieve_topk(q_vec, topk=TOPK)\n",
    "\n",
    "    prompt = build_prompt(processed_query, retrieved)\n",
    "\n",
    "    answer = generate_from_model(prompt)\n",
    "\n",
    "    if mask_map:\n",
    "        for tok, orig in mask_map.items():\n",
    "            answer = answer.replace(tok, orig)\n",
    "\n",
    "    rec_id = i\n",
    "    payload = {\n",
    "        \"id\": rec_id,\n",
    "        \"query\": query_text,\n",
    "        \"processed_query\": processed_query,\n",
    "        \"mask_map\": mask_map,\n",
    "        \"entities\": entities,\n",
    "        \"retrieved_docs\": retrieved,\n",
    "        \"final_answer\": answer,\n",
    "        \"consent\": consent_flag\n",
    "    }\n",
    "\n",
    "    write_encrypted_json(rec_id, payload)\n",
    "    results.append(payload)\n",
    "    print(f\"[{i}/{len(user_queries)}] Query processed. Consent={consent_flag}. Final answer length: {len(answer)} chars.\")\n",
    "\n",
    "\n",
    "OUT_PATH = os.path.join(BASE_DIR, \"user_query_results.json\")\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(results, fh, indent=2)\n",
    "\n",
    "print(f\"All results saved to {OUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}