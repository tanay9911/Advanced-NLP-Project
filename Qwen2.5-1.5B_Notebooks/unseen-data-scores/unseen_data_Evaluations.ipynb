{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install ragas sentence-transformers scikit-learn bert-score rouge-score sacrebleu\n"
   ],
   "metadata": {
    "id": "QvR6w0oqR2w1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bc719ce4c35e45aa86daa9c05abaf33e",
      "2691d755fdee4a3f8bdd9f9aba6cf924",
      "c225cb3fb09c43abbf562d472596478a",
      "e9a4a4e3846b4f2682665375b9e444b2",
      "6633acc29c0a4812863f3226fdadb891",
      "38c41447967f44f7bb02ee69456e0268",
      "5dd6ecc0fc4f4f219e94ee2357a511c4",
      "b79ece16674a413e9f57186da23c8122",
      "a75346de9e0a4ba78468f9d2f7917eef",
      "2ac6b6220929479c93510389c9517411",
      "4eae19ce414949798a5fff77eea8ee07",
      "8b3449289deb4f58adf6f8d9d5fe0379",
      "3a360530eb9c4c28abdf950b3c0538a8",
      "0f77df4d9e0043ee99e4776e941d8d64",
      "a82ef566502144d5aea571f7fb13679b",
      "792d21a7da0148bebf0ab146093f9130",
      "f71f196885064a0f99bf9c1596650623",
      "15ccf18ef0d246ddae0b2a64f6f751bd",
      "e69aa762f4e54243a25069f3fa0b1f77",
      "e5f5b81fa65646d1a7de66791391b4da",
      "8f7d1674c9f04811a052a18e48abf213",
      "10234b3f657b46ed8467f6b832535da4",
      "22a7a4dbd0d446679d8cfaa9012c06bf",
      "87d2cd342ab44fe29b7eeea9fe3050c2",
      "ea982a0bb3cf4b03830d9a91e486e917",
      "1b4f4c7735b549ce85d8ca4e9e6b57c6",
      "b1a57f5e4215443a837345ebae5d1e78",
      "d5dbae4fec5e4f0b93c6b70a8cefc561",
      "ec946de4849c45e785a4dcceea2f7bca",
      "1519cb649e314a03aa3b19fb5ad5d785",
      "80287baa384b459a8aac19fc25dadb29",
      "f716317edb9e4c8a8c55f3f40f85bd82",
      "333c85ea0fee418c90d52d40e89a6f28",
      "bdf69409e0f046dcbe59f21f8ca5f2a9",
      "7b0fb9d193ea40c08c79d1321b842725",
      "01e88568bcd14de893967e8b05e6bfb1",
      "ffce1bf912cb43049b11d391e5481dd1",
      "9e02dac81c0b47c0ab14b6a09afcc82f",
      "44a0097579654d3bb9ca438534aaefbc",
      "b473599a07564622baac5b5539dba2d1",
      "c70c435261a44a9180fa48be9d4168ce",
      "30c2090e6a4e475a97f87e95dc41952b",
      "c6cd82ce362144e8a824bfe4e869b4b5",
      "8a002068eb3245088e1927c210a53b82",
      "ca0cdb34b6ba4cf4928212802141de63",
      "5a0d4b50672340d58de16371421bc035",
      "208706ab47444eb79c14bd9151758d77",
      "785a95934970484a94e5eaba35fe82a4",
      "5a94aa0f919443d38697ebd7ea01d193",
      "fcfa83ec63ed4df19283a72fe9783f13",
      "2cbd7247620548fbbd982faf412a3c7e",
      "a2bea0d84b564814ac5b97d4e2f9af44",
      "cb8278c55d1044ffb2652fa86fd0a293",
      "d63f9e0e05c742fa9fdb8b9752b4a0a0",
      "edf4e43dcc2b4027a067107d704e7ff2",
      "5938f342dc3646a5b4d14f32ab2d372b",
      "6bb5ecbc5dde480187cb34213d5d806b",
      "a36e2d0400bd4b4e8508eab36dcc4534",
      "dcd01008696a4008b16410c2b37de4fd",
      "c16120b77d2e4a5f927f80465f123ebe",
      "99d3be09703e4b9fb724060f3c817bbb",
      "9fcc710652d949bd974dac285e971009",
      "9fd1ba6e6d39430ab013acbfe9f1af6a",
      "1585628ab2ce42d283c590702d63e5da",
      "f09a54c580874bab9c3841f1410b0768",
      "7bb4bf06ff204d99aa090584968270c2"
     ]
    },
    "id": "2gimliy2RmpJ",
    "outputId": "dfc4c041-1260-42d6-d875-913b59045b43"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc719ce4c35e45aa86daa9c05abaf33e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b3449289deb4f58adf6f8d9d5fe0379"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22a7a4dbd0d446679d8cfaa9012c06bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdf69409e0f046dcbe59f21f8ca5f2a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca0cdb34b6ba4cf4928212802141de63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5938f342dc3646a5b4d14f32ab2d372b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "RAG Metrics (Average)\n",
      "\n",
      "Retrieval_similarity_avg: 0.3241628714162727\n",
      "Semantic_similarity_avg: 0.596695714148468\n",
      "Faithfulness_avg: 0.3716007979993505\n",
      "Answer_relevancy_avg: 0.5076429891673013\n",
      "Bert_f1_avg: 0.8619098550286787\n",
      "RougeL_avg: 0.29714103922149687\n",
      "Bleu_avg: 0.11366154259983038\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed(texts):\n",
    "    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "def compute_all_metrics(gt_file, result_file):\n",
    "    gt_data = json.load(open(gt_file))\n",
    "    result_data = json.load(open(result_file))\n",
    "\n",
    "    gt_map = {item[\"query\"]: item[\"report\"] for item in gt_data}\n",
    "\n",
    "    retrieval_sims = []\n",
    "    semantic_sims  = []\n",
    "    faithfulness   = []\n",
    "    relevancy      = []\n",
    "    bert_f1_scores = []\n",
    "    rouge_scores   = []\n",
    "    bleu_scores    = []\n",
    "\n",
    "    for item in result_data:\n",
    "        original_query = item[\"original_query\"]\n",
    "        answer = item[\"final_answer\"]\n",
    "        retrieved_cases = item[\"retrieved_cases\"]\n",
    "\n",
    "        if original_query not in gt_map:\n",
    "            continue\n",
    "\n",
    "        gt_answer = gt_map[original_query]\n",
    "\n",
    "        retrieved_docs = [case[\"report\"] for case in retrieved_cases]\n",
    "\n",
    "        all_texts = [original_query, answer, gt_answer] + retrieved_docs\n",
    "        embeddings = embed(all_texts)\n",
    "\n",
    "        q_emb   = embeddings[0]\n",
    "        ans_emb = embeddings[1]\n",
    "        gt_emb  = embeddings[2]\n",
    "        retr_embs = embeddings[3:]\n",
    "\n",
    "        if len(retr_embs) > 0:\n",
    "            sim_vals = [float(util.cos_sim(q_emb, r)) for r in retr_embs]\n",
    "            retrieval_sims.append(np.mean(sim_vals))\n",
    "        else:\n",
    "            retrieval_sims.append(0.0)\n",
    "\n",
    "        semantic_sims.append(float(util.cos_sim(gt_emb, ans_emb)))\n",
    "\n",
    "        if len(retr_embs) > 0:\n",
    "            faith = [float(util.cos_sim(ans_emb, r)) for r in retr_embs]\n",
    "            faithfulness.append(np.mean(faith))\n",
    "        else:\n",
    "            faithfulness.append(0.0)\n",
    "\n",
    "        relevancy.append(float(util.cos_sim(q_emb, ans_emb)))\n",
    "\n",
    "        _, _, F = bert_score([answer], [gt_answer], lang=\"en\", verbose=False)\n",
    "        bert_f1_scores.append(float(F[0]))\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        r = scorer.score(gt_answer, answer)[\"rougeL\"].fmeasure\n",
    "        rouge_scores.append(r)\n",
    "\n",
    "        bleu = corpus_bleu([answer], [[gt_answer]]).score / 100.0\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return {\n",
    "        \"Retrieval_similarity_avg\": float(np.mean(retrieval_sims)),\n",
    "        \"Semantic_similarity_avg\": float(np.mean(semantic_sims)),\n",
    "        \"Faithfulness_avg\": float(np.mean(faithfulness)),\n",
    "        \"Answer_relevancy_avg\": float(np.mean(relevancy)),\n",
    "        \"Bert_f1_avg\": float(np.mean(bert_f1_scores)),\n",
    "        \"RougeL_avg\": float(np.mean(rouge_scores)),\n",
    "        \"Bleu_avg\": float(np.mean(bleu_scores))\n",
    "    }\n",
    "\n",
    "scores = compute_all_metrics(\n",
    "    \"/content/drive/MyDrive/unseen-data-cal/artifacts/unseen-rag-test.json\",\n",
    "    \"/content/drive/MyDrive/unseen-data-cal/encrypted_rag/user_query_results.json\"\n",
    ")\n",
    "print(\"\\nRAG Metrics (Average)\\n\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ]
}