{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuCgRPvIWzxn",
    "outputId": "28d8db4d-e22e-417c-9708-0d894eac111b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for negspacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence-transformers faiss-cpu ipywidgets cryptography sacremoses spacy negspacy bitsandbytes accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GL2mLcR37WU",
    "outputId": "6d428db0-c344-4125-f988-95b3d624a70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "og9eBgfZYMIM",
    "outputId": "24c20aa4-a6a4-46af-abb0-cac9d26306de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u26a0\ufe0f  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: write).\n",
      "The token `token9911` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `token9911`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6mwm7YPW92F"
   },
   "outputs": [],
   "source": [
    "import os, re, json, time, random, math, warnings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from cryptography.fernet import Fernet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/gemma_finetune\"\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "\n",
    "DATASET_JSON = os.path.join(DRIVE_BASE, \"dataset.json\")\n",
    "ARTIFACTS_DIR = os.path.join(DRIVE_BASE, \"artifacts\")\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define the split files\n",
    "RAG_TRAIN_JSON = os.path.join(ARTIFACTS_DIR, \"rag-train.json\")\n",
    "RAG_TEST_JSON = os.path.join(ARTIFACTS_DIR, \"rag-test.json\")\n",
    "\n",
    "# Fine-tuning files\n",
    "FINETUNE_TRAIN_JSONL = os.path.join(ARTIFACTS_DIR, \"finetune-train.jsonl\")\n",
    "FINETUNE_TEST_JSONL = os.path.join(ARTIFACTS_DIR, \"finetune-test.jsonl\")\n",
    "\n",
    "FAISS_PATH = os.path.join(ARTIFACTS_DIR, \"reports_index.faiss\")\n",
    "META_PATH = os.path.join(ARTIFACTS_DIR, \"reports_metadata.json\")\n",
    "\n",
    "FINETUNED_DIR = os.path.join(DRIVE_BASE, \"gemma2b_qlora_ft_merged\")\n",
    "os.makedirs(FINETUNED_DIR, exist_ok=True)\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "DEFAULT_BASE_MODEL = \"google/gemma-2b\"\n",
    "RANDOM_SEED = 42\n",
    "SIM_THRESHOLD = 0.30\n",
    "TOPK = 3\n",
    "MASK_PROB = 0.60\n",
    "TRAIN_SIZE = 0.80  # 80-20 split\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "LEARNING_RATE = 2e-4  \n",
    "MAX_LENGTH = 200\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy NER model.\")\n",
    "except Exception as e:\n",
    "    print(\"spaCy model not found, downloading...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy NER model.\")\n",
    "\n",
    "# Only mask names and personal info,\n",
    "PII_LABELS = {\"PERSON\", \"GPE\", \"LOC\"}\n",
    "\n",
    "def simple_regex_mask(text: str) -> str:\n",
    "    out = text\n",
    "    # Only mask names, emails, phones\n",
    "    out = re.sub(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", \"[NAME]\", out)\n",
    "    out = re.sub(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", \"[EMAIL]\", out)\n",
    "    out = re.sub(r\"\\b(\\+?\\d{1,3}[-.\\s]?)?(\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4})\\b\", \"[PHONE]\", out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def mask_with_spacy(text: str) -> Tuple[str, Dict[str,str]]:\n",
    "    doc = nlp(text)\n",
    "    mask_map = {}\n",
    "    masked = text\n",
    "\n",
    "    entities = sorted(doc.ents, key=lambda x: x.start_char, reverse=True)\n",
    "\n",
    "    for ent in entities:\n",
    "        # Only mask PERSON, GPE, LOC\n",
    "        if ent.label_ in PII_LABELS:\n",
    "            placeholder = f\"<{ent.label_}_{len(mask_map)+1}>\"\n",
    "            mask_map[placeholder] = ent.text\n",
    "            masked = masked[:ent.start_char] + placeholder + masked[ent.end_char:]\n",
    "\n",
    "    masked = simple_regex_mask(masked)\n",
    "\n",
    "    return masked, mask_map\n",
    "\n",
    "def apply_masking_to_dataset(records: List[Dict[str,str]], mask_prob: float = MASK_PROB) -> List[Dict[str,str]]:\n",
    "    \"\"\"Apply masking to a dataset with given probability\"\"\"\n",
    "    masked_records = []\n",
    "    masked_count = 0\n",
    "\n",
    "    for rec in records:\n",
    "        query = rec.get(\"query\", \"\").strip()\n",
    "        report = rec.get(\"report\", \"\").strip()\n",
    "\n",
    "        if not query or not report:\n",
    "            continue\n",
    "\n",
    "        # Apply masking with probability\n",
    "        do_mask = (random.random() < mask_prob)\n",
    "\n",
    "        if do_mask:\n",
    "            masked_query, mask_map = mask_with_spacy(query)\n",
    "            masked_records.append({\n",
    "                \"query\": masked_query,\n",
    "                \"report\": report,\n",
    "                \"original_query\": query,  # Keep original for reference\n",
    "                \"masked\": True\n",
    "            })\n",
    "            masked_count += 1\n",
    "        else:\n",
    "            masked_records.append({\n",
    "                \"query\": query,\n",
    "                \"report\": report,\n",
    "                \"masked\": False\n",
    "            })\n",
    "\n",
    "    print(f\"Applied masking: {masked_count}/{len(masked_records)} masked ({masked_count/len(masked_records)*100:.1f}%)\")\n",
    "    return masked_records\n",
    "\n",
    "# Create 80-20 split files WITH MASKING\n",
    "def create_rag_split_files():\n",
    "    \"\"\"Create rag-train.json and rag-test.json with 80-20 split AND MASKING\"\"\"\n",
    "    if not os.path.exists(DATASET_JSON):\n",
    "        raise FileNotFoundError(f\"Put your dataset.json at: {DATASET_JSON}\")\n",
    "\n",
    "    with open(DATASET_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "        raw_data = json.load(fh)\n",
    "\n",
    "    assert isinstance(raw_data, list), \"dataset.json must be a list of {query, report} pairs.\"\n",
    "    print(f\"Loaded {len(raw_data)} examples\")\n",
    "\n",
    "    # Create 80-20 split\n",
    "    train_data, test_data = train_test_split(raw_data, train_size=TRAIN_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "    # APPLY MASKING TO BOTH SPLITS\n",
    "    print(\"Applying masking to RAG train split...\")\n",
    "    train_data_masked = apply_masking_to_dataset(train_data, mask_prob=MASK_PROB)\n",
    "\n",
    "    print(\"Applying masking to RAG test split...\")\n",
    "    test_data_masked = apply_masking_to_dataset(test_data, mask_prob=MASK_PROB)\n",
    "\n",
    "    # Save the MASKED split files\n",
    "    with open(RAG_TRAIN_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(train_data_masked, fh, indent=2)\n",
    "\n",
    "    with open(RAG_TEST_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(test_data_masked, fh, indent=2)\n",
    "\n",
    "    print(f\"Created MASKED RAG split files:\")\n",
    "    print(f\"  - {RAG_TRAIN_JSON}: {len(train_data_masked)} examples (80%, {MASK_PROB*100}% masked)\")\n",
    "    print(f\"  - {RAG_TEST_JSON}: {len(test_data_masked)} examples (20%, {MASK_PROB*100}% masked)\")\n",
    "\n",
    "    return train_data_masked, test_data_masked\n",
    "\n",
    "# Load or create RAG split files\n",
    "if os.path.exists(RAG_TRAIN_JSON) and os.path.exists(RAG_TEST_JSON):\n",
    "    print(\"Loading existing MASKED RAG split files...\")\n",
    "    with open(RAG_TRAIN_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "        rag_train_data = json.load(fh)\n",
    "    with open(RAG_TEST_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
    "        rag_test_data = json.load(fh)\n",
    "else:\n",
    "    print(\"Creating new MASKED RAG split files...\")\n",
    "    rag_train_data, rag_test_data = create_rag_split_files()\n",
    "\n",
    "print(\"Loading sentence-transformers embedder...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device=device)\n",
    "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dim:\", EMB_DIM)\n",
    "\n",
    "print(\"Building FAISS index over MASKED RAG-TRAIN QUERIES...\")\n",
    "train_queries = [rec[\"query\"] for rec in rag_train_data]\n",
    "if len(train_queries) == 0:\n",
    "    raise RuntimeError(\"No queries found in rag-train split.\")\n",
    "\n",
    "query_embs = embedder.encode(train_queries, convert_to_numpy=True).astype(\"float32\")\n",
    "faiss.normalize_L2(query_embs)\n",
    "index = faiss.IndexFlatIP(EMB_DIM)\n",
    "index.add(query_embs)\n",
    "faiss.write_index(index, FAISS_PATH)\n",
    "\n",
    "# Metadata from MASKED rag-train for retrieval\n",
    "meta = [{\"id\": i, \"query\": rag_train_data[i][\"query\"], \"report\": rag_train_data[i][\"report\"]} for i in range(len(rag_train_data))]\n",
    "with open(META_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(meta, fh, indent=2)\n",
    "print(f\"FAISS index saved to {FAISS_PATH}. Metadata saved to {META_PATH}\")\n",
    "\n",
    "def retrieve_topk_query_pairs(current_query: str, current_idx: int = -1, topk: int = TOPK, sim_threshold: float = SIM_THRESHOLD) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar queries from MASKED RAG-TRAIN data\n",
    "    \"\"\"\n",
    "    fetch_k = max(10, topk * 4)\n",
    "\n",
    "    q_emb = embedder.encode([current_query], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, fetch_k)\n",
    "\n",
    "    hits = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(meta):\n",
    "            continue\n",
    "        if float(score) < sim_threshold:\n",
    "            continue\n",
    "\n",
    "        # Skip if this is the same exact query (Avoiding self-retrieval) \n",
    "        if current_idx != -1 and idx == current_idx:\n",
    "            continue\n",
    "\n",
    "        rec = meta[int(idx)]\n",
    "        hits.append({\n",
    "            \"similar_query\": rec[\"query\"],\n",
    "            \"report\": rec[\"report\"],\n",
    "            \"score\": float(score),\n",
    "            \"orig_idx\": int(idx)\n",
    "        })\n",
    "\n",
    "        if len(hits) >= topk:\n",
    "            break\n",
    "\n",
    "    return hits\n",
    "\n",
    "def build_prompt(query: str, retrieved_pairs: List[Dict[str,Any]]) -> str:\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(\"[QUERY]\")\n",
    "    prompt_lines.append(query.strip())\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"[SIMILAR_CASES]\")\n",
    "    for i, pair in enumerate(retrieved_pairs, start=1):\n",
    "        prompt_lines.append(f\"Case {i}:\")\n",
    "        prompt_lines.append(f\"Query: {pair['similar_query']}\")\n",
    "        prompt_lines.append(f\"Report: {pair['report']}\")\n",
    "        prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"[INSTRUCTION]\")\n",
    "    prompt_lines.append(\"Generate a clinical report. Do NOT include disclaimers.\")\n",
    "    prompt = \"\\n\".join(prompt_lines)\n",
    "    return prompt\n",
    "\n",
    "def create_finetune_jsonl(records: List[Dict[str,str]], out_path: str, apply_masking: bool = True, mask_prob: float = MASK_PROB, sim_threshold:float = SIM_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Create finetune JSONL files with retrieval from RAG-TRAIN data\n",
    "    \n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    skipped = 0\n",
    "    additional_masked_count = 0\n",
    "    sample_inputs = []\n",
    "\n",
    "    for idx, rec in enumerate(records):\n",
    "        original_query = rec.get(\"query\",\"\").strip()\n",
    "        report = rec.get(\"report\",\"\").strip()\n",
    "        if not original_query or not report:\n",
    "            continue\n",
    "\n",
    "        # For training data: pass index to avoid self-retrieval\n",
    "        # For test data: no self-retrieval concern since test data is not in index\n",
    "        current_idx = idx if apply_masking else -1\n",
    "\n",
    "        hits = retrieve_topk_query_pairs(original_query, current_idx, topk=TOPK, sim_threshold=sim_threshold)\n",
    "        if len(hits) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Use the query as-is (already masked from RAG files)\n",
    "        \n",
    "        final_query = original_query\n",
    "\n",
    "        # Apply additional masking for data augmentation\n",
    "        do_additional_mask = (random.random() < mask_prob) if apply_masking else False\n",
    "\n",
    "        if do_additional_mask:\n",
    "            # Re-mask the query create different masking patterns\n",
    "            final_query, mask_map = mask_with_spacy(rec.get(\"original_query\", original_query))\n",
    "            additional_masked_count += 1\n",
    "            mask_status = \"ADDITIONALLY_MASKED\"\n",
    "        else:\n",
    "            mask_status = \"MASKED\" if rec.get(\"masked\", False) else \"UNMASKED\"\n",
    "\n",
    "        prompt = build_prompt(final_query, hits)\n",
    "        obj = {\"input\": prompt, \"output\": report}\n",
    "        lines.append(json.dumps(obj, ensure_ascii=False))\n",
    "\n",
    "        if len(sample_inputs) < 3:\n",
    "            sample_inputs.append({\n",
    "                \"original_query\": rec.get(\"original_query\", original_query),\n",
    "                \"input_query\": final_query,\n",
    "                \"mask_status\": mask_status,\n",
    "                \"retrieved_count\": len(hits),\n",
    "                \"retrieved_queries\": [hit[\"similar_query\"][:50] + \"...\" for hit in hits],\n",
    "                \"final_prompt\": prompt[:500] + \"...\" if len(prompt) > 500 else prompt\n",
    "            })\n",
    "\n",
    "    print(f\"Wrote {len(lines)} examples to {out_path} (skipped {skipped} examples due to no retrieval hits).\")\n",
    "    if apply_masking:\n",
    "        print(f\"Additional masking applied: {additional_masked_count}/{len(lines)}\")\n",
    "\n",
    "    print(\"SAMPLE INPUTS THAT WILL GO TO LLM:\")\n",
    "    for i, sample in enumerate(sample_inputs, 1):\n",
    "        print(f\"SAMPLE {i} - {sample['mask_status']}:\")\n",
    "        print(f\"Original: {sample['original_query']}\")\n",
    "        print(f\"Input:    {sample['input_query']}\")\n",
    "        print(f\"Retrieved: {sample['retrieved_count']} cases from RAG-TRAIN\")\n",
    "        print(f\"Retrieved queries: {sample['retrieved_queries']}\")\n",
    "        print(f\"Prompt preview: {sample['final_prompt'][:200]}...\")\n",
    "        print()\n",
    "\n",
    "    # Write to file\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for line in lines:\n",
    "            fh.write(line + \"\\n\")\n",
    "\n",
    "    return len(lines)\n",
    "\n",
    "# Create finetune files - BOTH WITH MASKING\n",
    "print(\"Creating finetune-train.jsonl and finetune-test.jsonl ...\")\n",
    "n_train = create_finetune_jsonl(rag_train_data, FINETUNE_TRAIN_JSONL, apply_masking=True, mask_prob=MASK_PROB, sim_threshold=SIM_THRESHOLD)\n",
    "n_test = create_finetune_jsonl(rag_test_data, FINETUNE_TEST_JSONL, apply_masking=True, mask_prob=MASK_PROB, sim_threshold=SIM_THRESHOLD)\n",
    "\n",
    "# Verification\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "if os.path.exists(FINETUNE_TRAIN_JSONL):\n",
    "    print(f\" Finetune train file created: {FINETUNE_TRAIN_JSONL}\")\n",
    "    train_size = sum(1 for _ in open(FINETUNE_TRAIN_JSONL, 'r'))\n",
    "    print(f\" Finetune train examples: {train_size}\")\n",
    "else:\n",
    "    print(f\" Finetune train file missing: {FINETUNE_TRAIN_JSONL}\")\n",
    "\n",
    "if os.path.exists(FINETUNE_TEST_JSONL):\n",
    "    print(f\" Finetune test file created: {FINETUNE_TEST_JSONL}\")\n",
    "    test_size = sum(1 for _ in open(FINETUNE_TEST_JSONL, 'r'))\n",
    "    print(f\" Finetune test examples: {test_size}\")\n",
    "else:\n",
    "    print(f\" Finetune test file missing: {FINETUNE_TEST_JSONL}\")\n",
    "\n",
    "print(\"Sample training example:\")\n",
    "if os.path.exists(FINETUNE_TRAIN_JSONL):\n",
    "    with open(FINETUNE_TRAIN_JSONL, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            if i>0: break\n",
    "            print(json.dumps(json.loads(line), indent=2))\n",
    "else:\n",
    "    print(\"Finetune train file not available for preview\")\n",
    "\n",
    "\n",
    "print(\"Preparing for QLoRA fine-tuning...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<|pad|>\"})\n",
    "\n",
    "def load_jsonl_to_dataset(jsonl_path: str):\n",
    "    records = []\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        print(f\"Warning: {jsonl_path} not found\")\n",
    "        return Dataset.from_list([])\n",
    "\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            ln = ln.strip()\n",
    "            if not ln: continue\n",
    "            obj = json.loads(ln)\n",
    "            records.append(obj)\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "# Load from finetune files\n",
    "train_ds = load_jsonl_to_dataset(FINETUNE_TRAIN_JSONL)\n",
    "val_ds = load_jsonl_to_dataset(FINETUNE_TEST_JSONL)  # Using test as validation\n",
    "print(\"Train/examples:\", len(train_ds), \"Val/examples:\", len(val_ds))\n",
    "\n",
    "# Skip training if no data\n",
    "if len(train_ds) == 0:\n",
    "    print(\"ERROR: No training data found. Check file paths and Drive sync.\")\n",
    "    exit()\n",
    "\n",
    "def tokenize_and_mask(example):\n",
    "    inp = example[\"input\"]\n",
    "    tgt = example[\"output\"]\n",
    "    full_text = inp + \"\\n\\n\" + tgt + tokenizer.eos_token\n",
    "    tokenized_full = tokenizer(full_text, truncation=True, max_length=MAX_LENGTH)\n",
    "    input_tokenized = tokenizer(inp, truncation=True, max_length=MAX_LENGTH)\n",
    "    input_len = len(input_tokenized[\"input_ids\"])\n",
    "    labels = tokenized_full[\"input_ids\"].copy()\n",
    "    for i in range(input_len):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "    tokenized_full[\"labels\"] = labels\n",
    "    return tokenized_full\n",
    "\n",
    "train_ds_tokenized = train_ds.map(tokenize_and_mask, remove_columns=train_ds.column_names, num_proc=1)\n",
    "val_ds_tokenized = val_ds.map(tokenize_and_mask, remove_columns=val_ds.column_names, num_proc=1)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "\n",
    "print(\"Loading base model in 8-bit...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "target_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"]\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA adapters added. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_dir = os.path.join(ARTIFACTS_DIR, \"qlora_runs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200 if len(train_ds_tokenized)>200 else max(1, len(train_ds_tokenized)//5),\n",
    "    save_steps=200 if len(train_ds_tokenized)>200 else max(1, len(train_ds_tokenized)//5),\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.03,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    push_to_hub=False,\n",
    "   \n",
    "    max_grad_norm=1.0,              \n",
    "    dataloader_pin_memory=False,   \n",
    "    gradient_checkpointing=True,    \n",
    "    remove_unused_columns=False,    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_tokenized,\n",
    "    eval_dataset=val_ds_tokenized if len(val_ds_tokenized)>0 else None,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Applied fixes: gradient clipping (max_grad_norm=1.0), lower learning rate, gradient checkpointing\")\n",
    "trainer.train()\n",
    "\n",
    "lora_weights_dir = os.path.join(ARTIFACTS_DIR, \"lora_weights\")\n",
    "os.makedirs(lora_weights_dir, exist_ok=True)\n",
    "model.save_pretrained(lora_weights_dir)\n",
    "print(\"Saved LoRA weights to:\", lora_weights_dir)\n",
    "\n",
    "print(\"Merging LoRA adapters into base model ...\")\n",
    "try:\n",
    "    model = model.merge_and_unload()\n",
    "except Exception as e:\n",
    "    print(\"merge_and_unload() not available or failed:\", e)\n",
    "\n",
    "print(\"Saving final model to:\", FINETUNED_DIR)\n",
    "model.save_pretrained(FINETUNED_DIR)\n",
    "tokenizer.save_pretrained(FINETUNED_DIR)\n",
    "print(\"Final fine-tuned model saved to Drive at:\", FINETUNED_DIR)\n",
    "\n",
    "# Update artifacts manifest\n",
    "artifacts_info = {\n",
    "    \"rag_train\": RAG_TRAIN_JSON,\n",
    "    \"rag_test\": RAG_TEST_JSON,\n",
    "    \"finetune_train\": FINETUNE_TRAIN_JSONL,\n",
    "    \"finetune_test\": FINETUNE_TEST_JSONL,\n",
    "    \"faiss_index\": FAISS_PATH,\n",
    "    \"meta\": META_PATH,\n",
    "    \"lora_weights\": lora_weights_dir,\n",
    "    \"merged_model\": FINETUNED_DIR,\n",
    "    \"notes\": {\n",
    "        \"split_ratio\": \"80-20\",\n",
    "        \"mask_probability\": MASK_PROB,\n",
    "        \"sim_threshold\": SIM_THRESHOLD,\n",
    "        \"topk\": TOPK,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"stability_fixes\": \"gradient_clipping, lower_lr, checkpointing\",\n",
    "        \"description\": \"ALL datasets contain 60% masked queries for consistent training and evaluation\"\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"artifacts_manifest.json\"), \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(artifacts_info, fh, indent=2)\n",
    "\n",
    "\n",
    "print(f\"  RAG Train: {RAG_TRAIN_JSON} (60% masked)\")\n",
    "print(f\"   RAG Test: {RAG_TEST_JSON} (60% masked)\")\n",
    "print(f\"   Finetune Train: {FINETUNE_TRAIN_JSONL} (60% masked)\")\n",
    "print(f\"   Finetune Test: {FINETUNE_TEST_JSONL} (60% masked)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}