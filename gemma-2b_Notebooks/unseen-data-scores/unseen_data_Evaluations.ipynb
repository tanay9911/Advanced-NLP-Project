{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install ragas sentence-transformers scikit-learn bert-score rouge-score sacrebleu"
   ],
   "metadata": {
    "id": "p03EszLef-73"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "387791ce7f64463d9eec30e40a9c6a9e",
      "33175813186948b6b0dd391cb4639b90",
      "831ab6d66e874d68a907c92a108dbe44",
      "52aad9cb16604afbaba33ae1303f5b90",
      "49fae3c0bf88403d8e761a59d912f3f7",
      "bb1bc173fb0544019daad7fcc730fec6",
      "9215f90f37614f1eb7d887c778981ff7",
      "4ea54a4fbb1d45609bc4d597c884e175",
      "dfb7f5347d7d484a9d35ae5c5079da81",
      "b5bce1ff25914120b014154c47c4a1ea",
      "39898e9fcd6e4b9a8d673b7e891c5a42",
      "c195ddbe589043e6976cece290d22d5e",
      "59e234f8a04740709ebbaf457cbd4055",
      "ba0bfe0ce4474218906a2d3c9a751d4d",
      "0316797ca5d14008bf8af92475e2a516",
      "6ee5e9756cd1477baef0f12b3ee68c5d",
      "718d28c6a6ef4192988e3804904c5dcd",
      "f7c19ba3a6a54bcf93436dafdf0bd167",
      "0665e88a0d0a4577bbbebe9e194891f5",
      "41df0212438a4fc7aaed9384dbc8b124",
      "313d6e4eded74fb7ada7d805232ad429",
      "be0a4fe7aec140109e9748739f70509f",
      "c1912b3effa944fb8adf046974971a82",
      "ea0e547ea3f84768b48cdde5d67369e9",
      "83d21a57476a4f71977a64f2d1a15a22",
      "be84d3a3231f4465b3f0162d252398f3",
      "4f733e709e724c7f95f6536451c1f8c9",
      "8babe0d01a48407e9c7b6b087f116125",
      "2e00c8732a2b43cdaca35fee00fc45e2",
      "2626ffc6ecc5471cbc002edd768b57f7",
      "7421d12962d74475b5ea1652c715f931",
      "8137200f196d47ceae695af5dc29e9a1",
      "0148e8953d1d493594cfcd2f53f202c4",
      "f3a1d279e3ff418f96caed7652535bd8",
      "1df660110176431bb502c1ae229e792b",
      "504f8bf1f3a845d0afc53e145b67b6e4",
      "1d81d79cce0f4f4683de66cb05a1269a",
      "a040f30a8bce43d9b72747b473462707",
      "f0c09b37d6c24c3a99dd68cc61ee2b28",
      "041864afed934d97a3b7df3f6dbe45af",
      "40d11119e9ce433ab36a003061de8107",
      "2e52d91c31ab489daed047e62f95723f",
      "7f4df873a58546c4a46ae8372d7656c0",
      "2f18977c2b624f26abe81c123833789f",
      "ed116855059949c6afb1c96fac6ac72a",
      "6b197cd600b443038a773ba0b30395f0",
      "0ea9dcd187d44053ad9de299196c3c38",
      "458adff2097749668087b361ffc1f026",
      "3ad8a2defbc84699b420ffacd0bccbc5",
      "9258fe904b974626b508c04f9cf775ff",
      "9ead74dd3b9249828c3f64e9956357b2",
      "8528edc5dbef4d5c844f2907625894a9",
      "0ea737abf66446c7a6bc24311608e63f",
      "03a146f714314ee89f34c02de1976b9a",
      "9aa22613abab4874a0605a44a23eb234",
      "e2c0007f87a4451e8e6c89b0fee7c54c",
      "0b3c8bfdb7614794b52d4066c3c53946",
      "a677942ac37e4e6ebaa96ad8a1dc8cc6",
      "4e45c2cff7b043b9b61e52bc1262063c",
      "b43c31fc8e0a47559081af896dc97a35",
      "cfdc1a7bacb14ad0a7f95130b64d3709",
      "a5d4058cb06f4a67924f67fe91a41e9b",
      "5b6c79ab4df042e5874899507c0f00c7",
      "5979f9cd2bac4025993ff9e1c8fc9b35",
      "7521732f226a4eb88d87ad836dc2049b",
      "b8435bde6fc74cfab88ae836188f7340"
     ]
    },
    "id": "ohEs4KxVeu9v",
    "outputId": "ff572d23-b08f-4f11-a7ab-03a836c49907"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "387791ce7f64463d9eec30e40a9c6a9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c195ddbe589043e6976cece290d22d5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1912b3effa944fb8adf046974971a82"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3a1d279e3ff418f96caed7652535bd8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed116855059949c6afb1c96fac6ac72a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2c0007f87a4451e8e6c89b0fee7c54c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "RAG Metrics (Average)\n",
      "\n",
      "Retrieval_similarity_avg: 0.3227225282689793\n",
      "Semantic_similarity_avg: 0.4856590917735393\n",
      "Faithfulness_avg: 0.35407420515414634\n",
      "Answer_relevancy_avg: 0.4654246450498186\n",
      "Bert_f1_avg: 0.8558173282393093\n",
      "RougeL_avg: 0.1994248781002695\n",
      "Bleu_avg: 0.07236397141212116\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed(texts):\n",
    "    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "def compute_all_metrics(gt_file, result_file):\n",
    "    gt_data = json.load(open(gt_file))\n",
    "    result_data = json.load(open(result_file))\n",
    "\n",
    "    gt_map = {item[\"query\"]: item[\"report\"] for item in gt_data}\n",
    "\n",
    "    retrieval_sims = []\n",
    "    semantic_sims  = []\n",
    "    faithfulness   = []\n",
    "    relevancy      = []\n",
    "    bert_f1_scores = []\n",
    "    rouge_scores   = []\n",
    "    bleu_scores    = []\n",
    "\n",
    "    for item in result_data:\n",
    "        original_query = item[\"original_query\"]\n",
    "        answer = item[\"final_answer\"]\n",
    "        retrieved_cases = item[\"retrieved_cases\"]\n",
    "\n",
    "        if original_query not in gt_map:\n",
    "            continue\n",
    "\n",
    "        gt_answer = gt_map[original_query]\n",
    "\n",
    "        retrieved_docs = [case[\"report\"] for case in retrieved_cases]\n",
    "\n",
    "        all_texts = [original_query, answer, gt_answer] + retrieved_docs\n",
    "        embeddings = embed(all_texts)\n",
    "\n",
    "        q_emb   = embeddings[0]\n",
    "        ans_emb = embeddings[1]\n",
    "        gt_emb  = embeddings[2]\n",
    "        retr_embs = embeddings[3:]\n",
    "\n",
    "        if len(retr_embs) > 0:\n",
    "            sim_vals = [float(util.cos_sim(q_emb, r)) for r in retr_embs]\n",
    "            retrieval_sims.append(np.mean(sim_vals))\n",
    "        else:\n",
    "            retrieval_sims.append(0.0)\n",
    "\n",
    "        semantic_sims.append(float(util.cos_sim(gt_emb, ans_emb)))\n",
    "\n",
    "        if len(retr_embs) > 0:\n",
    "            faith = [float(util.cos_sim(ans_emb, r)) for r in retr_embs]\n",
    "            faithfulness.append(np.mean(faith))\n",
    "        else:\n",
    "            faithfulness.append(0.0)\n",
    "\n",
    "        relevancy.append(float(util.cos_sim(q_emb, ans_emb)))\n",
    "\n",
    "        _, _, F = bert_score([answer], [gt_answer], lang=\"en\", verbose=False)\n",
    "        bert_f1_scores.append(float(F[0]))\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        r = scorer.score(gt_answer, answer)[\"rougeL\"].fmeasure\n",
    "        rouge_scores.append(r)\n",
    "\n",
    "        bleu = corpus_bleu([answer], [[gt_answer]]).score / 100.0\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return {\n",
    "        \"Retrieval_similarity_avg\": float(np.mean(retrieval_sims)),\n",
    "        \"Semantic_similarity_avg\": float(np.mean(semantic_sims)),\n",
    "        \"Faithfulness_avg\": float(np.mean(faithfulness)),\n",
    "        \"Answer_relevancy_avg\": float(np.mean(relevancy)),\n",
    "        \"Bert_f1_avg\": float(np.mean(bert_f1_scores)),\n",
    "        \"RougeL_avg\": float(np.mean(rouge_scores)),\n",
    "        \"Bleu_avg\": float(np.mean(bleu_scores))\n",
    "    }\n",
    "\n",
    "scores = compute_all_metrics(\n",
    "    \"/content/drive/MyDrive/unseen-data-cal/artifacts/unseen-rag-test.json\",\n",
    "    \"/content/drive/MyDrive/unseen-data-cal/encrypted_rag/user_query_results.json\"\n",
    ")\n",
    "print(\"\\nRAG Metrics (Average)\\n\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ]
}