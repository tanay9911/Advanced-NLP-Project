{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "DATASET_JSON = \"/content/drive/MyDrive/gemma_finetune/dataset.json\"\n",
        "RAG_JSON = \"/content/drive/MyDrive/gemma_finetune/rag-data.json\"\n",
        "USER_QUERY_JSON = \"/content/drive/MyDrive/gemma_finetune/user-query.json\"\n",
        "\n",
        "# Load original dataset\n",
        "with open(DATASET_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Shuffle for random split\n",
        "random.shuffle(dataset)\n",
        "\n",
        "# 70% RAG, 30% user queries\n",
        "split_idx = int(len(dataset) * 0.7)\n",
        "rag_data = dataset[:split_idx]\n",
        "user_queries = dataset[split_idx:]\n",
        "\n",
        "# Save RAG data\n",
        "with open(RAG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rag_data, f, indent=2)\n",
        "print(f\"RAG data saved: {len(rag_data)} items -> {RAG_JSON}\")\n",
        "\n",
        "# Save user queries\n",
        "with open(USER_QUERY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(user_queries, f, indent=2)\n",
        "print(f\"User queries saved: {len(user_queries)} items -> {USER_QUERY_JSON}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs-Lno5eq29f",
        "outputId": "ee7b2b76-c87e-41c4-fd8a-0fa46024e877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG data saved: 334 items -> /content/drive/MyDrive/gemma_finetune/rag-data.json\n",
            "User queries saved: 144 items -> /content/drive/MyDrive/gemma_finetune/user-query.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu ipywidgets cryptography sacremoses spacy negspacy bitsandbytes accelerate -q"
      ],
      "metadata": {
        "id": "ohLbtsd3sDO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeLod9tTcdAE",
        "outputId": "42d855e8-b24e-4019-807c-d6cc9847a60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Encrypted RAG already exists.\n",
            "Loaded 334 RAG documents.\n",
            "Embedding dim: 384\n",
            "FAISS index built and saved at /content/drive/MyDrive/User_data_rag/rag_faiss.index\n",
            "Fine-tuned model loaded in 8-bit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
            "[2/144] Query processed. Consent=False. Final answer length: 122 chars.\n",
            "[3/144] Query processed. Consent=True. Final answer length: 86 chars.\n",
            "[4/144] Query processed. Consent=False. Final answer length: 96 chars.\n",
            "[5/144] Query processed. Consent=False. Final answer length: 121 chars.\n",
            "[6/144] Query processed. Consent=False. Final answer length: 89 chars.\n",
            "[7/144] Query processed. Consent=False. Final answer length: 88 chars.\n",
            "[8/144] Query processed. Consent=False. Final answer length: 94 chars.\n",
            "[9/144] Query processed. Consent=True. Final answer length: 76 chars.\n",
            "[10/144] Query processed. Consent=False. Final answer length: 112 chars.\n",
            "[11/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
            "[12/144] Query processed. Consent=True. Final answer length: 108 chars.\n",
            "[13/144] Query processed. Consent=True. Final answer length: 82 chars.\n",
            "[14/144] Query processed. Consent=False. Final answer length: 117 chars.\n",
            "[15/144] Query processed. Consent=False. Final answer length: 98 chars.\n",
            "[16/144] Query processed. Consent=False. Final answer length: 96 chars.\n",
            "[17/144] Query processed. Consent=True. Final answer length: 96 chars.\n",
            "[18/144] Query processed. Consent=False. Final answer length: 118 chars.\n",
            "[19/144] Query processed. Consent=True. Final answer length: 69 chars.\n",
            "[20/144] Query processed. Consent=True. Final answer length: 92 chars.\n",
            "[21/144] Query processed. Consent=True. Final answer length: 110 chars.\n",
            "[22/144] Query processed. Consent=True. Final answer length: 108 chars.\n",
            "[23/144] Query processed. Consent=False. Final answer length: 89 chars.\n",
            "[24/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
            "[25/144] Query processed. Consent=False. Final answer length: 89 chars.\n",
            "[26/144] Query processed. Consent=False. Final answer length: 84 chars.\n",
            "[27/144] Query processed. Consent=False. Final answer length: 104 chars.\n",
            "[28/144] Query processed. Consent=True. Final answer length: 69 chars.\n",
            "[29/144] Query processed. Consent=True. Final answer length: 79 chars.\n",
            "[30/144] Query processed. Consent=True. Final answer length: 67 chars.\n",
            "[31/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
            "[32/144] Query processed. Consent=True. Final answer length: 73 chars.\n",
            "[33/144] Query processed. Consent=True. Final answer length: 41 chars.\n",
            "[34/144] Query processed. Consent=False. Final answer length: 83 chars.\n",
            "[35/144] Query processed. Consent=False. Final answer length: 90 chars.\n",
            "[36/144] Query processed. Consent=False. Final answer length: 71 chars.\n",
            "[37/144] Query processed. Consent=True. Final answer length: 81 chars.\n",
            "[38/144] Query processed. Consent=True. Final answer length: 81 chars.\n",
            "[39/144] Query processed. Consent=False. Final answer length: 95 chars.\n",
            "[40/144] Query processed. Consent=False. Final answer length: 93 chars.\n",
            "[41/144] Query processed. Consent=True. Final answer length: 113 chars.\n",
            "[42/144] Query processed. Consent=True. Final answer length: 100 chars.\n",
            "[43/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
            "[44/144] Query processed. Consent=False. Final answer length: 67 chars.\n",
            "[45/144] Query processed. Consent=True. Final answer length: 82 chars.\n",
            "[46/144] Query processed. Consent=True. Final answer length: 116 chars.\n",
            "[47/144] Query processed. Consent=True. Final answer length: 103 chars.\n",
            "[48/144] Query processed. Consent=False. Final answer length: 112 chars.\n",
            "[49/144] Query processed. Consent=True. Final answer length: 85 chars.\n",
            "[50/144] Query processed. Consent=False. Final answer length: 96 chars.\n",
            "[51/144] Query processed. Consent=True. Final answer length: 105 chars.\n",
            "[52/144] Query processed. Consent=False. Final answer length: 67 chars.\n",
            "[53/144] Query processed. Consent=False. Final answer length: 80 chars.\n",
            "[54/144] Query processed. Consent=False. Final answer length: 55 chars.\n",
            "[55/144] Query processed. Consent=False. Final answer length: 80 chars.\n",
            "[56/144] Query processed. Consent=True. Final answer length: 99 chars.\n",
            "[57/144] Query processed. Consent=True. Final answer length: 62 chars.\n",
            "[58/144] Query processed. Consent=True. Final answer length: 109 chars.\n",
            "[59/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
            "[60/144] Query processed. Consent=False. Final answer length: 71 chars.\n",
            "[61/144] Query processed. Consent=True. Final answer length: 76 chars.\n",
            "[62/144] Query processed. Consent=False. Final answer length: 40 chars.\n",
            "[63/144] Query processed. Consent=False. Final answer length: 75 chars.\n",
            "[64/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
            "[65/144] Query processed. Consent=False. Final answer length: 71 chars.\n",
            "[66/144] Query processed. Consent=True. Final answer length: 115 chars.\n",
            "[67/144] Query processed. Consent=True. Final answer length: 66 chars.\n",
            "[68/144] Query processed. Consent=True. Final answer length: 92 chars.\n",
            "[69/144] Query processed. Consent=True. Final answer length: 112 chars.\n",
            "[70/144] Query processed. Consent=True. Final answer length: 98 chars.\n",
            "[71/144] Query processed. Consent=False. Final answer length: 90 chars.\n",
            "[72/144] Query processed. Consent=False. Final answer length: 123 chars.\n",
            "[73/144] Query processed. Consent=True. Final answer length: 94 chars.\n",
            "[74/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
            "[75/144] Query processed. Consent=True. Final answer length: 112 chars.\n",
            "[76/144] Query processed. Consent=False. Final answer length: 73 chars.\n",
            "[77/144] Query processed. Consent=False. Final answer length: 107 chars.\n",
            "[78/144] Query processed. Consent=True. Final answer length: 57 chars.\n",
            "[79/144] Query processed. Consent=False. Final answer length: 82 chars.\n",
            "[80/144] Query processed. Consent=False. Final answer length: 114 chars.\n",
            "[81/144] Query processed. Consent=False. Final answer length: 70 chars.\n",
            "[82/144] Query processed. Consent=True. Final answer length: 73 chars.\n",
            "[83/144] Query processed. Consent=False. Final answer length: 116 chars.\n",
            "[84/144] Query processed. Consent=False. Final answer length: 60 chars.\n",
            "[85/144] Query processed. Consent=True. Final answer length: 84 chars.\n",
            "[86/144] Query processed. Consent=True. Final answer length: 117 chars.\n",
            "[87/144] Query processed. Consent=False. Final answer length: 106 chars.\n",
            "[88/144] Query processed. Consent=False. Final answer length: 81 chars.\n",
            "[89/144] Query processed. Consent=True. Final answer length: 84 chars.\n",
            "[90/144] Query processed. Consent=False. Final answer length: 87 chars.\n",
            "[91/144] Query processed. Consent=False. Final answer length: 78 chars.\n",
            "[92/144] Query processed. Consent=False. Final answer length: 111 chars.\n",
            "[93/144] Query processed. Consent=False. Final answer length: 110 chars.\n",
            "[94/144] Query processed. Consent=False. Final answer length: 101 chars.\n",
            "[95/144] Query processed. Consent=False. Final answer length: 60 chars.\n",
            "[96/144] Query processed. Consent=True. Final answer length: 118 chars.\n",
            "[97/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
            "[98/144] Query processed. Consent=False. Final answer length: 72 chars.\n",
            "[99/144] Query processed. Consent=False. Final answer length: 105 chars.\n",
            "[100/144] Query processed. Consent=False. Final answer length: 121 chars.\n",
            "[101/144] Query processed. Consent=False. Final answer length: 128 chars.\n",
            "[102/144] Query processed. Consent=False. Final answer length: 120 chars.\n",
            "[103/144] Query processed. Consent=False. Final answer length: 97 chars.\n",
            "[104/144] Query processed. Consent=False. Final answer length: 112 chars.\n",
            "[105/144] Query processed. Consent=False. Final answer length: 83 chars.\n",
            "[106/144] Query processed. Consent=True. Final answer length: 95 chars.\n",
            "[107/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
            "[108/144] Query processed. Consent=True. Final answer length: 86 chars.\n",
            "[109/144] Query processed. Consent=False. Final answer length: 77 chars.\n",
            "[110/144] Query processed. Consent=True. Final answer length: 85 chars.\n",
            "[111/144] Query processed. Consent=False. Final answer length: 80 chars.\n",
            "[112/144] Query processed. Consent=False. Final answer length: 65 chars.\n",
            "[113/144] Query processed. Consent=False. Final answer length: 67 chars.\n",
            "[114/144] Query processed. Consent=True. Final answer length: 106 chars.\n",
            "[115/144] Query processed. Consent=False. Final answer length: 70 chars.\n",
            "[116/144] Query processed. Consent=False. Final answer length: 83 chars.\n",
            "[117/144] Query processed. Consent=False. Final answer length: 127 chars.\n",
            "[118/144] Query processed. Consent=False. Final answer length: 87 chars.\n",
            "[119/144] Query processed. Consent=False. Final answer length: 116 chars.\n",
            "[120/144] Query processed. Consent=True. Final answer length: 84 chars.\n",
            "[121/144] Query processed. Consent=False. Final answer length: 62 chars.\n",
            "[122/144] Query processed. Consent=False. Final answer length: 112 chars.\n",
            "[123/144] Query processed. Consent=False. Final answer length: 103 chars.\n",
            "[124/144] Query processed. Consent=False. Final answer length: 60 chars.\n",
            "[125/144] Query processed. Consent=False. Final answer length: 115 chars.\n",
            "[126/144] Query processed. Consent=True. Final answer length: 87 chars.\n",
            "[127/144] Query processed. Consent=False. Final answer length: 83 chars.\n",
            "[128/144] Query processed. Consent=False. Final answer length: 90 chars.\n",
            "[129/144] Query processed. Consent=True. Final answer length: 100 chars.\n",
            "[130/144] Query processed. Consent=False. Final answer length: 88 chars.\n",
            "[131/144] Query processed. Consent=True. Final answer length: 78 chars.\n",
            "[132/144] Query processed. Consent=True. Final answer length: 42 chars.\n",
            "[133/144] Query processed. Consent=True. Final answer length: 116 chars.\n",
            "[134/144] Query processed. Consent=False. Final answer length: 121 chars.\n",
            "[135/144] Query processed. Consent=True. Final answer length: 90 chars.\n",
            "[136/144] Query processed. Consent=False. Final answer length: 94 chars.\n",
            "[137/144] Query processed. Consent=False. Final answer length: 113 chars.\n",
            "[138/144] Query processed. Consent=True. Final answer length: 42 chars.\n",
            "[139/144] Query processed. Consent=False. Final answer length: 82 chars.\n",
            "[140/144] Query processed. Consent=False. Final answer length: 109 chars.\n",
            "[141/144] Query processed. Consent=True. Final answer length: 77 chars.\n",
            "[142/144] Query processed. Consent=False. Final answer length: 84 chars.\n",
            "[143/144] Query processed. Consent=True. Final answer length: 117 chars.\n",
            "[144/144] Query processed. Consent=True. Final answer length: 81 chars.\n",
            "All results saved to /content/drive/MyDrive/User_data_rag/user_query_results.json\n"
          ]
        }
      ],
      "source": [
        "# Imports & Setup\n",
        "\n",
        "import os, json, re, time, random, warnings\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from cryptography.fernet import Fernet\n",
        "\n",
        "\n",
        "# Paths & configuration\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/User_data_rag\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "RAG_JSON = \"/content/drive/MyDrive/gemma_finetune/rag-data.json\"\n",
        "ENCRYPTED_RAG_PATH = os.path.join(BASE_DIR, \"rag_data.json.enc\")\n",
        "USER_QUERY_JSON = \"/content/drive/MyDrive/gemma_finetune/user-query.json\"\n",
        "FINETUNED_MODEL_DIR = \"/content/drive/MyDrive/gemma_finetune/gemma2b_qlora_ft_merged\"\n",
        "\n",
        "FAISS_INDEX_PATH = os.path.join(BASE_DIR, \"rag_faiss.index\")\n",
        "FERNET_KEY_PATH = os.path.join(BASE_DIR, \"fernet.key\")\n",
        "AUDIT_LOG_PATH = os.path.join(BASE_DIR, \"audit.log\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "TOPK = 3\n",
        "SIM_THRESHOLD = 0.3\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "PII_LABELS = {\"PERSON\",\"PATIENT\",\"LOCATION\",\"ADDRESS\",\"EMAIL\",\"PHONE\",\"ID\",\"DATE\"}\n",
        "MEDICAL_ENTS = {\"MEDICATION\",\"DISEASE\",\"DISEASES\",\"SYMPTOM\",\"SYMPTOMS\",\"SIGN_SYMPTOM\",\"PROCEDURE\",\"LABS\",\"VITALS\",\"AGE\"}\n",
        "\n",
        "\n",
        "# Encryption helpers\n",
        "\n",
        "def get_or_create_fernet_key(path: str = FERNET_KEY_PATH) -> bytes:\n",
        "    if os.path.exists(path):\n",
        "        return open(path,\"rb\").read()\n",
        "    key = Fernet.generate_key()\n",
        "    open(path,\"wb\").write(key)\n",
        "    return key\n",
        "\n",
        "FERNET_KEY = get_or_create_fernet_key()\n",
        "FERNET = Fernet(FERNET_KEY)\n",
        "\n",
        "def encrypt_bytes(b: bytes) -> bytes:\n",
        "    return FERNET.encrypt(b)\n",
        "\n",
        "def decrypt_bytes(b: bytes) -> bytes:\n",
        "    return FERNET.decrypt(b)\n",
        "\n",
        "def write_encrypted_json(id_int: int, payload: Dict[str,Any]) -> str:\n",
        "    path = os.path.join(BASE_DIR, f\"{id_int}.json.enc\")\n",
        "    raw = json.dumps(payload, default=str).encode(\"utf-8\")\n",
        "    cipher = encrypt_bytes(raw)\n",
        "    with open(path,\"wb\") as fh:\n",
        "        fh.write(cipher)\n",
        "    return path\n",
        "\n",
        "def write_encrypted_rag(rag_data: List[Dict[str,Any]], path: str = ENCRYPTED_RAG_PATH):\n",
        "    raw = json.dumps(rag_data, default=str).encode(\"utf-8\")\n",
        "    with open(path,\"wb\") as fh:\n",
        "        fh.write(encrypt_bytes(raw))\n",
        "    print(f\"Encrypted RAG saved to {path}\")\n",
        "\n",
        "def load_decrypted_rag(path: str = ENCRYPTED_RAG_PATH) -> List[Dict[str,Any]]:\n",
        "    with open(path,\"rb\") as fh:\n",
        "        decrypted = decrypt_bytes(fh.read())\n",
        "    return json.loads(decrypted)\n",
        "\n",
        "\n",
        "# Encrypt RAG data if not already encrypted\n",
        "\n",
        "if not os.path.exists(ENCRYPTED_RAG_PATH):\n",
        "    with open(RAG_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
        "        rag_data = json.load(fh)\n",
        "    write_encrypted_rag(rag_data)\n",
        "else:\n",
        "    print(\"Encrypted RAG already exists.\")\n",
        "\n",
        "# Load decrypted RAG and build FAISS\n",
        "\n",
        "rag_data = load_decrypted_rag()\n",
        "print(f\"Loaded {len(rag_data)} RAG documents.\")\n",
        "\n",
        "embedder = SentenceTransformer(EMBED_MODEL, device=DEVICE)\n",
        "EMB_DIM = embedder.get_sentence_embedding_dimension()\n",
        "print(\"Embedding dim:\", EMB_DIM)\n",
        "\n",
        "index = faiss.IndexFlatIP(EMB_DIM)\n",
        "texts = [rec[\"report\"] for rec in rag_data]\n",
        "embs = embedder.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
        "faiss.normalize_L2(embs)\n",
        "index.add(embs)\n",
        "faiss.write_index(index, FAISS_INDEX_PATH)\n",
        "print(f\"FAISS index built and saved at {FAISS_INDEX_PATH}\")\n",
        "\n",
        "# Store metadata\n",
        "META_PATH = os.path.join(BASE_DIR, \"rag_metadata.json\")\n",
        "meta = [{\"id\": i, \"report\": rag_data[i][\"report\"], \"query\": rag_data[i].get(\"query\",\"\")} for i in range(len(rag_data))]\n",
        "with open(META_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(meta, fh, indent=2)\n",
        "\n",
        "\n",
        "# Load fine-tuned Gemma2 model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    FINETUNED_MODEL_DIR,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned model loaded in 8-bit.\")\n",
        "\n",
        "\n",
        "# NER + simple PII masking\n",
        "\n",
        "try:\n",
        "    ner_pipe = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    ner_pipe = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def simple_mask(text: str) -> Tuple[str, Dict[str,str]]:\n",
        "    mask_map = {}\n",
        "    out = text\n",
        "    # names\n",
        "    names = re.findall(r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\", out)\n",
        "    for i,n in enumerate(names,1):\n",
        "        token = f\"<PHI_PERSON_{i}>\"; mask_map[token]=n\n",
        "        out = out.replace(n, token)\n",
        "    # emails\n",
        "    emails = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", out)\n",
        "    for i,e in enumerate(emails,1):\n",
        "        token = f\"<PHI_EMAIL_{i}>\"; mask_map[token]=e\n",
        "        out = out.replace(e, token)\n",
        "    # phones\n",
        "    phones = re.findall(r\"\\b(\\+?\\d[\\d\\-\\s]{7,}\\d)\\b\", out)\n",
        "    for i,p in enumerate(phones,1):\n",
        "        token = f\"<PHI_PHONE_{i}>\"; mask_map[token]=p\n",
        "        out = out.replace(p, token)\n",
        "    # dates\n",
        "    dates = re.findall(r\"\\b\\d{1,2}[\\/\\-\\.\\s]\\d{1,2}[\\/\\-\\.\\s]\\d{2,4}\\b\", out)\n",
        "    for i,d in enumerate(dates,1):\n",
        "        token = f\"<PHI_DATE_{i}>\"; mask_map[token]=d\n",
        "        out = out.replace(d, token)\n",
        "    return out, mask_map\n",
        "\n",
        "\n",
        "# Entity extraction helper\n",
        "\n",
        "def extract_entities(text: str) -> Dict[str,Any]:\n",
        "    doc = ner_pipe(text)\n",
        "    ents = {}\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_.upper()\n",
        "        ents.setdefault(label, []).append({\"text\":ent.text,\"start\":ent.start_char,\"end\":ent.end_char})\n",
        "    return ents\n",
        "\n",
        "\n",
        "# Negation marking\n",
        "\n",
        "NEG_WORDS = r\"\\b(no|not|denies?|without|absent|negative for)\\b\"\n",
        "def mark_negation(entities: Dict[str,Any], text: str, window_chars:int=40) -> Dict[str,Any]:\n",
        "    for label, recs in entities.items():\n",
        "        new_recs=[]\n",
        "        for r in recs:\n",
        "            r[\"negated\"]=False\n",
        "            s = r.get(\"start\",0)\n",
        "            context = text[max(0,s-window_chars):s].lower()\n",
        "            if re.search(NEG_WORDS, context):\n",
        "                r[\"negated\"]=True\n",
        "            new_recs.append(r)\n",
        "        entities[label]=new_recs\n",
        "    return entities\n",
        "\n",
        "# RAG retrieval\n",
        "\n",
        "def retrieve_topk(query_vec: np.ndarray, topk:int=TOPK, sim_threshold:float=SIM_THRESHOLD) -> List[Dict[str,Any]]:\n",
        "    q = query_vec.reshape(1,-1).astype(\"float32\")\n",
        "    faiss.normalize_L2(q)\n",
        "    D,I = index.search(q, topk*4)\n",
        "    retrieved=[]\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx < 0 or idx >= len(meta):\n",
        "            continue\n",
        "        if score < sim_threshold:\n",
        "            continue\n",
        "        retrieved.append(meta[int(idx)])\n",
        "        if len(retrieved)>=topk:\n",
        "            break\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "# Prompt builder (training format EXACT)\n",
        "\n",
        "def build_prompt(query: str, retrieved: List[Dict[str,Any]]) -> str:\n",
        "    lines = [\"[QUERY]\", query, \"\", \"[RETRIEVED_CONTEXT]\"]\n",
        "\n",
        "    for i, r in enumerate(retrieved, 1):\n",
        "        lines.append(f\"{i}. {r['report']}\")\n",
        "\n",
        "    for i in range(len(retrieved) + 1, TOPK + 1):\n",
        "        lines.append(f\"{i}. \")\n",
        "\n",
        "    lines.extend([\n",
        "        \"\",\n",
        "        \"[INSTRUCTION]\",\n",
        "        \"Generate a clinical report. Do NOT include disclaimers.\"\n",
        "    ])\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# STRICT FIX â€” remove prompt echoing\n",
        "\n",
        "def generate_from_model(prompt: str, max_tokens: int = 256) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )[0]\n",
        "\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    gen_ids = out[prompt_len:]              # ONLY NEW TOKENS\n",
        "\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Load user queries and run pipeline with random 60% masked / 40% unmasked\n",
        "\n",
        "with open(USER_QUERY_JSON, \"r\", encoding=\"utf-8\") as fh:\n",
        "    user_queries = json.load(fh)\n",
        "\n",
        "results = []\n",
        "num_queries = len(user_queries)\n",
        "num_masked = int(num_queries * 0.6)\n",
        "\n",
        "shuffled_indices = list(range(num_queries))\n",
        "random.shuffle(shuffled_indices)\n",
        "masked_indices = set(shuffled_indices[:num_masked])\n",
        "\n",
        "for i, rec in enumerate(user_queries,1):\n",
        "    query_text = rec.get(\"query\",\"\").strip()\n",
        "    consent_flag = False if (i-1) in masked_indices else True\n",
        "\n",
        "    entities = extract_entities(query_text)\n",
        "    entities = mark_negation(entities, query_text)\n",
        "\n",
        "    if not consent_flag:\n",
        "        processed_query, mask_map = simple_mask(query_text)\n",
        "    else:\n",
        "        processed_query = query_text\n",
        "        mask_map = {}\n",
        "\n",
        "    q_vec = embedder.encode([processed_query], convert_to_numpy=True).astype(\"float32\")[0]\n",
        "\n",
        "    retrieved = retrieve_topk(q_vec, topk=TOPK)\n",
        "\n",
        "    prompt = build_prompt(processed_query, retrieved)\n",
        "\n",
        "    answer = generate_from_model(prompt)\n",
        "\n",
        "    if mask_map:\n",
        "        for tok, orig in mask_map.items():\n",
        "            answer = answer.replace(tok, orig)   # FIXED DIRECTION\n",
        "\n",
        "    rec_id = i\n",
        "    payload = {\n",
        "        \"id\": rec_id,\n",
        "        \"query\": query_text,\n",
        "        \"processed_query\": processed_query,\n",
        "        \"mask_map\": mask_map,\n",
        "        \"entities\": entities,\n",
        "        \"retrieved_docs\": retrieved,\n",
        "        \"final_answer\": answer,\n",
        "        \"consent\": consent_flag\n",
        "    }\n",
        "    write_encrypted_json(rec_id, payload)\n",
        "    results.append(payload)\n",
        "    print(f\"[{i}/{len(user_queries)}] Query processed. Consent={consent_flag}. Final answer length: {len(answer)} chars.\")\n",
        "\n",
        "OUT_PATH = os.path.join(BASE_DIR, \"user_query_results.json\")\n",
        "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump(results, fh, indent=2)\n",
        "print(f\"All results saved to {OUT_PATH}\")\n"
      ]
    }
  ]
}